#!/usr/bin/env python3
"""
Twitter Watchdog - ä¸‰å±‚æ¶æ„ AI æ¨æ–‡ç›‘æ§å·¥å…·

æ¶æ„ï¼ˆä¸‰å±‚åˆ†ç¦»ï¼‰ï¼š
  Layer 1 - æŠ“å– (scrape):  twitterapi.io â†’ åŸå§‹æ¨æ–‡ â†’ output/raw/*.json
  Layer 2 - åˆ†æ (analyze): raw JSON â†’ Claude AI ç­›é€‰+æ€»ç»“ â†’ output/analysis/*.json
  Layer 3 - æŠ¥å‘Š (report):  analysis JSON â†’ HTML/MD æŠ¥å‘Š â†’ output/reports/*.{html,md}

æ•°æ®æºï¼š
  - X å®˜æ–¹ API (Bearer Token) â†’ è·å–å…³æ³¨åˆ—è¡¨ï¼ˆå…è´¹ï¼Œå¸¦æœ¬åœ°ç¼“å­˜ï¼‰
  - twitterapi.io â†’ æŠ“å–æ¨æ–‡å†…å®¹ï¼ˆ$0.15/1kï¼Œç»æµå®æƒ ï¼‰
  - Claude API â†’ AI æ™ºèƒ½ç­›é€‰+æ€»ç»“

æ—¶åŒºï¼šAsia/Shanghai (UTC+8)

ç”¨æ³•ï¼š
  # Layer 1: åªæŠ“å–ï¼Œå­˜åŸå§‹æ•°æ®
  python3 twitter_watchdog.py scrape [--hours-ago 6]

  # Layer 2: åˆ†æåŸå§‹æ•°æ®ï¼Œç”Ÿæˆåˆ†æç»“æœ
  python3 twitter_watchdog.py analyze [--hours-ago 6]
  python3 twitter_watchdog.py analyze --source raw/20260212_140000.json
  python3 twitter_watchdog.py analyze --from "2026-02-12 08:00" --to "2026-02-12 14:00"

  # Layer 3: ä»åˆ†æç»“æœç”ŸæˆæŠ¥å‘Š
  python3 twitter_watchdog.py report [--source analysis/20260212_143000.json]
  python3 twitter_watchdog.py report --daily 2026-02-12
  python3 twitter_watchdog.py report --weekly 2026-02-10
  python3 twitter_watchdog.py report --monthly 2026-02

  # æµæ°´çº¿æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼Œç­‰ä»·äº scrape + analyze + reportï¼‰
  python3 twitter_watchdog.py [--hours-ago 6]
"""

import os
import json
import yaml
import requests
import base64
import argparse
from datetime import datetime, timezone, timedelta
from pathlib import Path
import subprocess
import hashlib
import time
import re

# ä¸Šæµ·æ—¶åŒº UTC+8
TZ_CN = timezone(timedelta(hours=8))


class TwitterWatchdog:
    def __init__(self, config_file=None, cli_args=None, report_only=False):
        """åˆå§‹åŒ– Twitter Watchdog

        Args:
            report_only: True æ—¶è·³è¿‡ Twitter API åˆå§‹åŒ–ï¼ˆä»…ç”¨äºç”Ÿæˆå‘¨æŠ¥/æœˆæŠ¥ï¼‰
        """
        self.config = self.load_config(config_file)
        self.hours_ago = None

        # åº”ç”¨ CLI å‚æ•°è¦†ç›–
        if cli_args:
            self.apply_cli_overrides(cli_args)

        self.twitter_config = self.config["twitter"]
        self.output_config = self.config["output"]
        self.filters_config = self.config.get("filters", {})
        self.notifications_config = self.config.get("notifications", {})
        self.advanced_config = self.config.get("advanced", {})

        if not report_only:
            # twitterapi.io å‡­è¯ï¼ˆç”¨äºæŠ“å–æ¨æ–‡ + è·å–å…³æ³¨åˆ—è¡¨ï¼‰
            self.twitterapi_io_key = self.config.get("twitterapi_io", {}).get("api_key", "")

            # X å®˜æ–¹ API å‡­è¯ï¼ˆå¯é€‰ï¼Œä½œä¸ºè·å–å…³æ³¨åˆ—è¡¨çš„ fallbackï¼‰
            api_config = self.config.get("twitter_api", {})
            self.consumer_key = api_config.get("consumer_key", "")
            self.consumer_secret = api_config.get("consumer_secret", "")

            # ä»…åœ¨é…ç½®äº† X å®˜æ–¹å‡­è¯æ—¶ç”Ÿæˆ Bearer Token
            self.bearer_token = None
            if self.consumer_key and self.consumer_secret:
                try:
                    self.bearer_token = self._generate_bearer_token()
                except Exception as e:
                    print(f"  X å®˜æ–¹ API ä¸å¯ç”¨ï¼ˆ{e}ï¼‰ï¼Œå°†ä½¿ç”¨ twitterapi.io è·å–å…³æ³¨åˆ—è¡¨")

            self.timeout = self.advanced_config.get("timeout_seconds", 30)
            self.state = self.load_state()

        # æ¨æ–‡å›¾ç‰‡ URL æ˜ å°„: tweet_url -> image_urlï¼ˆæ‰€æœ‰æ¨¡å¼éƒ½éœ€è¦ï¼‰
        self.tweet_images = {}

    def apply_cli_overrides(self, args):
        """å°† CLI å‚æ•°è¦†ç›–åˆ° config"""
        if getattr(args, "hours_ago", None) is not None:
            self.hours_ago = args.hours_ago
        if getattr(args, "max_followings", None) is not None:
            self.config.setdefault("advanced", {})["max_followings"] = args.max_followings
        if getattr(args, "tweets_per_user", None) is not None:
            self.config.setdefault("twitter", {})["tweets_per_user"] = args.tweets_per_user
        if getattr(args, "trending_count", None) is not None:
            self.config.setdefault("trending_search", {})["max_tweets"] = args.trending_count
        if getattr(args, "trending_query", None) is not None:
            self.config.setdefault("trending_search", {})["query"] = args.trending_query
        if getattr(args, "min_faves", None) is not None:
            self.config.setdefault("trending_search", {})["min_views"] = args.min_faves
        if getattr(args, "language", None) is not None:
            self.config.setdefault("filters", {})["language"] = args.language
        if getattr(args, "exclude_users", None):
            self.config.setdefault("twitter", {})["exclude_users"] = [
                u.strip() for u in args.exclude_users.split(",")
            ]
        if getattr(args, "output_dir", None) is not None:
            self.config.setdefault("output", {})["directory"] = args.output_dir
        if getattr(args, "no_trending", False):
            self.config.setdefault("trending_search", {})["enabled"] = False
        if getattr(args, "no_summary", False):
            self.config.setdefault("ai_summary", {})["enabled"] = False

    def _generate_bearer_token(self):
        """é€šè¿‡ Consumer Key/Secret ç”Ÿæˆ Bearer Tokenï¼ˆX å®˜æ–¹ APIï¼‰"""
        credentials = base64.b64encode(
            f"{self.consumer_key}:{self.consumer_secret}".encode()
        ).decode()
        resp = requests.post(
            "https://api.twitter.com/oauth2/token",
            headers={
                "Authorization": f"Basic {credentials}",
                "Content-Type": "application/x-www-form-urlencoded;charset=UTF-8",
            },
            data="grant_type=client_credentials",
            timeout=15,
        )
        resp.raise_for_status()
        return resp.json()["access_token"]

    # â”€â”€ æ—¶é—´å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def now():
        """å½“å‰æ—¶é—´ï¼ˆUTC+8ï¼‰"""
        return datetime.now(TZ_CN)

    @staticmethod
    def parse_tweet_time(created_at):
        """è§£æ twitterapi.io çš„ createdAt å­—æ®µï¼Œè¿”å› aware datetime"""
        if not created_at:
            return None
        # æ ¼å¼: "Sat Feb 07 11:01:48 +0000 2026"
        try:
            dt = datetime.strptime(created_at, "%a %b %d %H:%M:%S %z %Y")
            return dt.astimezone(TZ_CN)
        except ValueError:
            pass
        # ISO æ ¼å¼å…œåº•
        try:
            dt = datetime.fromisoformat(created_at)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(TZ_CN)
        except ValueError:
            return None

    def is_tweet_in_window(self, tweet):
        """æ£€æŸ¥æ¨æ–‡æ˜¯å¦åœ¨ --hours-ago æ—¶é—´çª—å£å†…"""
        if self.hours_ago is None:
            return True
        created = self.parse_tweet_time(tweet.get("createdAt", ""))
        if created is None:
            return True  # æ— æ³•è§£ææ—¶é—´åˆ™ä¿ç•™
        cutoff = self.now() - timedelta(hours=self.hours_ago)
        return created >= cutoff

    # â”€â”€ é…ç½®ä¸çŠ¶æ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def load_config(self, config_file=None):
        if config_file is None:
            config_file = Path(__file__).parent.parent / "config" / "config.yaml"
        with open(config_file, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)

    def load_state(self):
        state_file = Path(
            self.advanced_config.get("state_file", ".twitter_watchdog_state.json")
        )
        if state_file.exists():
            with open(state_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                data["seen_tweets"] = set(data.get("seen_tweets", []))
                return data
        return {"seen_tweets": set(), "followings_cache": None, "followings_updated": None}

    def save_state(self):
        state_file = self.advanced_config.get(
            "state_file", ".twitter_watchdog_state.json"
        )
        state_to_save = {
            "seen_tweets": list(self.state["seen_tweets"]),
            "followings_cache": self.state.get("followings_cache"),
            "followings_updated": self.state.get("followings_updated"),
        }
        with open(state_file, "w", encoding="utf-8") as f:
            json.dump(state_to_save, f, ensure_ascii=False, indent=2)

    # â”€â”€ å»é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_tweet_hash(self, tweet_id):
        return hashlib.md5(str(tweet_id).encode()).hexdigest()

    def is_tweet_seen(self, tweet_id):
        return self.get_tweet_hash(tweet_id) in self.state["seen_tweets"]

    def mark_tweet_seen(self, tweet_id):
        self.state["seen_tweets"].add(self.get_tweet_hash(tweet_id))

    # â”€â”€ è·å–å…³æ³¨åˆ—è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _get_followings_twitterapiio(self):
        """é€šè¿‡ twitterapi.io è·å–å…³æ³¨åˆ—è¡¨ï¼ˆæ— éœ€ X å¼€å‘è€…è´¦å·ã€æ— éœ€ VPNï¼‰"""
        username = self.twitter_config["username"]
        print(f"  ä» twitterapi.io è·å– @{username} çš„å…³æ³¨åˆ—è¡¨...")
        max_followings = self.advanced_config.get("max_followings", 0)
        all_followings = []
        cursor = ""
        page = 0
        while True:
            page += 1
            params = {"userName": username, "pageSize": 200}
            if cursor:
                params["cursor"] = cursor
            data = self._twitterapiio_get("user/followings", params)
            batch = data.get("followings", [])
            # ç»Ÿä¸€å­—æ®µåä¸ºä¸ X å®˜æ–¹ API å…¼å®¹çš„æ ¼å¼
            for u in batch:
                u.setdefault("username", u.get("userName") or u.get("screen_name", ""))
                u.setdefault("name", u.get("name", u.get("username", "")))
                u.setdefault("description", u.get("description", ""))
                u.setdefault("public_metrics", {
                    "followers_count": u.get("followers_count", u.get("followers", 0))
                })
            all_followings.extend(batch)
            print(f"  ç¬¬ {page} é¡µ: +{len(batch)} (å…± {len(all_followings)})")
            if max_followings > 0 and len(all_followings) >= max_followings:
                all_followings = all_followings[:max_followings]
                break
            if not data.get("has_next_page") or not data.get("next_cursor"):
                break
            cursor = data["next_cursor"]
        return all_followings

    def _get_followings_x_api(self):
        """é€šè¿‡ X å®˜æ–¹ API è·å–å…³æ³¨åˆ—è¡¨ï¼ˆéœ€è¦å¼€å‘è€…è´¦å· + å¯èƒ½éœ€è¦ VPNï¼‰"""
        username = self.twitter_config["username"]
        print(f"  ä» X å®˜æ–¹ API è·å– @{username} çš„å…³æ³¨åˆ—è¡¨...")
        headers = {"Authorization": f"Bearer {self.bearer_token}"}
        retry = self.advanced_config.get("retry_attempts", 3)

        def x_api_get(url, params=None):
            for attempt in range(retry):
                resp = requests.get(url, headers=headers, params=params, timeout=self.timeout)
                if resp.status_code == 429:
                    reset = int(resp.headers.get("x-rate-limit-reset", 0))
                    wait = max(reset - int(time.time()), 10)
                    print(f"  X API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait}s...")
                    time.sleep(wait)
                    continue
                resp.raise_for_status()
                return resp.json()
            raise Exception("X API è¯·æ±‚å¤±è´¥")

        user_data = x_api_get(f"https://api.twitter.com/2/users/by/username/{username}")
        user_id = user_data["data"]["id"]

        max_followings = self.advanced_config.get("max_followings", 0)
        all_followings = []
        pagination_token = None
        while True:
            params = {"max_results": 1000, "user.fields": "username,name,description,public_metrics"}
            if pagination_token:
                params["pagination_token"] = pagination_token
            data = x_api_get(f"https://api.twitter.com/2/users/{user_id}/following", params)
            all_followings.extend(data.get("data", []))
            if max_followings > 0 and len(all_followings) >= max_followings:
                all_followings = all_followings[:max_followings]
                break
            pagination_token = data.get("meta", {}).get("next_token")
            if not pagination_token:
                break
        return all_followings

    def get_following(self):
        """è·å–å…³æ³¨åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼Œä¼˜å…ˆ twitterapi.ioï¼ŒX å®˜æ–¹ API ä½œä¸º fallbackï¼‰"""
        cache_hours = self.advanced_config.get("followings_cache_hours", 24)
        cached_time = self.state.get("followings_updated")
        cached_data = self.state.get("followings_cache")

        if cached_data and cached_time:
            try:
                updated = datetime.fromisoformat(cached_time)
                if self.now() - updated.astimezone(TZ_CN) < timedelta(hours=cache_hours):
                    print(f"  ä½¿ç”¨ç¼“å­˜çš„å…³æ³¨åˆ—è¡¨ï¼ˆ{len(cached_data)} äººï¼‰")
                    return cached_data
            except (ValueError, TypeError):
                pass

        # ä¼˜å…ˆä½¿ç”¨ twitterapi.ioï¼ˆæ— éœ€ VPNã€æ— éœ€ X å¼€å‘è€…è´¦å·ï¼‰
        all_followings = None
        if self.twitterapi_io_key:
            try:
                all_followings = self._get_followings_twitterapiio()
            except Exception as e:
                print(f"  twitterapi.io å…³æ³¨åˆ—è¡¨è·å–å¤±è´¥: {e}")

        # Fallback: X å®˜æ–¹ API
        if all_followings is None and self.bearer_token:
            try:
                all_followings = self._get_followings_x_api()
            except Exception as e:
                print(f"  X å®˜æ–¹ API è·å–å…³æ³¨åˆ—è¡¨å¤±è´¥: {e}")

        if all_followings is None:
            print("  é”™è¯¯: æ— æ³•è·å–å…³æ³¨åˆ—è¡¨")
            return []

        self.state["followings_cache"] = all_followings
        self.state["followings_updated"] = self.now().isoformat()

        exclude = set(self.twitter_config.get("exclude_users", []))
        if exclude:
            all_followings = [u for u in all_followings if u.get("username", "") not in exclude]
            print(f"  æ’é™¤ {len(exclude)} ä¸ªç”¨æˆ·åå‰©ä½™ {len(all_followings)} äºº")

        return all_followings

    # â”€â”€ twitterapi.ioï¼ˆæŠ“å–æ¨æ–‡ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _twitterapiio_get(self, endpoint, params=None):
        """twitterapi.io API è¯·æ±‚"""
        url = f"https://api.twitterapi.io/twitter/{endpoint}"
        headers = {"X-API-Key": self.twitterapi_io_key}
        retry = self.advanced_config.get("retry_attempts", 3)
        for attempt in range(retry):
            resp = requests.get(url, headers=headers, params=params, timeout=self.timeout)
            if resp.status_code == 429:
                print(f"  twitterapi.io é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… 10s...")
                time.sleep(10)
                continue
            resp.raise_for_status()
            return resp.json()
        raise Exception("twitterapi.io è¯·æ±‚å¤±è´¥")

    def get_tweets(self, username):
        """é€šè¿‡ twitterapi.io è·å–æŒ‡å®šç”¨æˆ·çš„æœ€æ–°æ¨æ–‡ï¼ˆæ”¯æŒåˆ†é¡µï¼‰

        å½“è®¾ç½®äº† hours_ago æ—¶ï¼Œä¼šè‡ªåŠ¨ç¿»é¡µç›´åˆ°æ¨æ–‡æ—¶é—´è¶…å‡ºæ—¶é—´çª—å£ã€‚
        è¿”å› (filtered_tweets, api_call_count) å…ƒç»„ã€‚
        """
        exclude_rt = self.twitter_config.get("exclude_retweets", True)
        exclude_reply = self.twitter_config.get("exclude_replies", True)
        max_tweets = self.twitter_config.get("tweets_per_user", 20)

        # æ—¶é—´çª—å£ cutoffï¼ˆç”¨äºå†³å®šæ˜¯å¦ç»§ç»­ç¿»é¡µï¼‰
        cutoff = None
        if self.hours_ago is not None:
            cutoff = self.now() - timedelta(hours=self.hours_ago)

        filtered = []
        cursor = ""
        api_call_count = 0
        max_pages = 10  # å®‰å…¨ä¸Šé™ï¼Œé˜²æ­¢æ— é™ç¿»é¡µ

        for _ in range(max_pages):
            params = {"userName": username}
            if cursor:
                params["cursor"] = cursor

            data = self._twitterapiio_get("user/last_tweets", params)
            api_call_count += 1

            tweets = data.get("data", {}).get("tweets", [])
            if not tweets:
                tweets = data.get("tweets", [])
            if not tweets:
                break

            oldest_in_page = None
            for t in tweets:
                if exclude_rt and (t.get("type") == "retweet" or t.get("text", "").startswith("RT @")):
                    continue
                if exclude_reply and t.get("isReply", False):
                    continue
                filtered.append(t)

                # è®°å½•æœ¬é¡µæœ€æ—§æ¨æ–‡æ—¶é—´
                created = self.parse_tweet_time(t.get("createdAt", ""))
                if created and (oldest_in_page is None or created < oldest_in_page):
                    oldest_in_page = created

            # ä¸éœ€è¦åˆ†é¡µçš„æƒ…å†µï¼šæ²¡è®¾ç½®æ—¶é—´çª—å£ï¼Œæˆ–å·²å¤Ÿæ•°
            if cutoff is None:
                break

            # æœ¬é¡µæœ€æ—§æ¨æ–‡å·²è¶…å‡ºæ—¶é—´çª—å£ï¼Œä¸éœ€è¦ç»§ç»­ç¿»é¡µ
            if oldest_in_page and oldest_in_page < cutoff:
                break

            # æ²¡æœ‰ä¸‹ä¸€é¡µ
            has_next = data.get("data", {}).get("has_next_page", data.get("has_next_page", False))
            next_cursor = data.get("data", {}).get("next_cursor", data.get("next_cursor", ""))
            if not has_next or not next_cursor:
                break
            cursor = next_cursor

        return filtered[:max_tweets], api_call_count

    @staticmethod
    def extract_media_url(tweet):
        """ä»æ¨æ–‡ä¸­æå–ç¬¬ä¸€å¼ å›¾ç‰‡ URL"""
        media_list = tweet.get("extendedEntities", {}).get("media", [])
        if not media_list:
            media_list = tweet.get("entities", {}).get("media", [])
        if not media_list:
            media_list = tweet.get("media", [])
        for m in media_list:
            url = m.get("media_url_https") or m.get("media_url") or m.get("url", "")
            if url and any(ext in url for ext in [".jpg", ".png", ".jpeg", ".gif", ".webp"]):
                return url
        return None

    def collect_tweet_image(self, tweet):
        """æ”¶é›†æ¨æ–‡çš„å›¾ç‰‡ URL åˆ° self.tweet_images"""
        tweet_url = tweet.get("url", "")
        if not tweet_url:
            return
        img = self.extract_media_url(tweet)
        if not img:
            # å¼•ç”¨æ¨æ–‡çš„å›¾ç‰‡
            quoted = tweet.get("quoted_tweet") or tweet.get("quotedTweet")
            if quoted:
                img = self.extract_media_url(quoted)
        if img:
            self.tweet_images[tweet_url] = img

    def download_report_images(self, summary_text, output_path):
        """ä¸‹è½½æŠ¥å‘Šä¸­å‡ºç°çš„æ¨æ–‡å›¾ç‰‡ï¼Œè¿”å› tweet_url -> relative_path æ˜ å°„"""
        if not summary_text or not self.tweet_images:
            return {}
        # è§£ææŠ¥å‘Šä¸­æ‰€æœ‰æ¨æ–‡ URL
        report_urls = re.findall(r'\(https://x\.com/[^)]+\)', summary_text)
        report_urls = [u.strip('()') for u in report_urls]
        urls_to_download = {u: self.tweet_images[u] for u in report_urls if u in self.tweet_images}
        if not urls_to_download:
            return {}

        ts = self.now().strftime("%Y%m%d_%H%M%S")
        img_dir = output_path / "images" / ts
        img_dir.mkdir(parents=True, exist_ok=True)
        downloaded = {}
        for tweet_url, img_url in urls_to_download.items():
            try:
                r = requests.get(img_url, timeout=15)
                r.raise_for_status()
                tid = tweet_url.rstrip("/").split("/")[-1]
                ext = ".png" if ".png" in img_url else ".gif" if ".gif" in img_url else ".jpg"
                fname = f"{tid}{ext}"
                with open(img_dir / fname, "wb") as f:
                    f.write(r.content)
                downloaded[tweet_url] = f"images/{ts}/{fname}"
            except Exception:
                pass
        if downloaded:
            print(f"  ä¸‹è½½å›¾ç‰‡: {len(downloaded)}/{len(urls_to_download)}")
        return downloaded

    @staticmethod
    def insert_images_into_summary(summary_text, downloaded):
        """åœ¨æŠ¥å‘Šä¸­æ¯æ¡æ¨æ–‡åæ’å…¥å›¾ç‰‡å¼•ç”¨"""
        if not downloaded:
            return summary_text
        lines = summary_text.split("\n")
        new_lines = []
        for line in lines:
            new_lines.append(line)
            for tweet_url, img_path in downloaded.items():
                if f"]({tweet_url})" in line:
                    new_lines.append(f"\n  ![tweet]({img_path})\n")
                    break
        return "\n".join(new_lines)

    # â”€â”€ å…¨ç½‘çƒ­é—¨ AI æœç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def search_trending_ai(self, max_tweets=20):
        """é€šè¿‡ Advanced Search æœç´¢å…¨ç½‘çƒ­é—¨ AI æ¨æ–‡"""
        search_config = self.config.get("trending_search", {})
        query = search_config.get(
            "query",
            "(AI OR LLM OR GPT OR Claude OR OpenAI OR AGI OR å¤§æ¨¡å‹) min_faves:50 -is:retweet -is:reply",
        )
        min_views = search_config.get("min_views", 2000)

        data = self._twitterapiio_get(
            "tweet/advanced_search",
            params={"query": query, "queryType": "Top"},
        )
        tweets = data.get("tweets", []) or data.get("data", {}).get("tweets", [])

        result = [t for t in tweets if t.get("viewCount", 0) >= min_views]
        result.sort(key=lambda t: t.get("viewCount", 0), reverse=True)
        return result[:max_tweets]

    # â”€â”€ è¿‡æ»¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def filter_tweet(self, tweet):
        """æ ¹æ®é…ç½®è¿‡æ»¤æ¨æ–‡"""
        if not self.filters_config.get("enabled", True):
            return True, "no_filters"

        lang_filter = self.filters_config.get("language", "all")
        if lang_filter != "all" and tweet.get("lang") != lang_filter:
            return False, "language_filter"

        min_likes = self.filters_config.get("min_likes", 0)
        min_retweets = self.filters_config.get("min_retweets", 0)
        if tweet.get("likeCount", 0) < min_likes:
            return False, "engagement_filter"
        if tweet.get("retweetCount", 0) < min_retweets:
            return False, "engagement_filter"

        # AI è¿‡æ»¤æ¨¡å¼ï¼šè·³è¿‡å…³é”®è¯åŒ¹é…ï¼Œç”± Claude åˆ¤æ–­ç›¸å…³æ€§
        ai_filter = self.config.get("ai_summary", {}).get("ai_filter", False)
        if ai_filter:
            return True, "ai_filter_mode"

        text = tweet.get("text", "").lower()
        exclude_keywords = self.filters_config.get("keywords", {}).get("exclude", [])
        for kw in exclude_keywords:
            if kw.lower() in text:
                return False, f"excluded_keyword:{kw}"

        include_keywords = self.filters_config.get("keywords", {}).get("include", [])
        if include_keywords:
            if not any(kw.lower() in text for kw in include_keywords):
                return False, "no_include_keyword"

        return True, "passed"

    # â”€â”€ Claude AI æ€»ç»“ + æ™ºèƒ½ç­›é€‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_style_prompts(self, style="standard", custom_prompt=""):
        """æ ¹æ® style é…ç½®ç”Ÿæˆ category_block å’Œ rules_block"""
        # åˆ†ç±»ç»“æ„ï¼ˆæ‰€æœ‰ style å…±ç”¨ï¼‰
        categories = """## æœ¬æœŸè¦ç‚¹

ç”¨ 3~5 ä¸ª bullet point æ¦‚æ‹¬æœ€é‡è¦çš„äº‹ä»¶/å‘å¸ƒ/è¶‹åŠ¿ï¼Œæ¯æ¡ä¸€å¥è¯ï¼Œä¸å¸¦é“¾æ¥ã€‚

## AI äº§å“ä¸å·¥å…·

æ–°äº§å“å‘å¸ƒã€äº§å“é‡å¤§æ›´æ–°ã€å·¥å…·æ¨èç­‰ã€‚

## AI æ¨¡å‹ä¸æŠ€æœ¯

æ–°æ¨¡å‹å‘å¸ƒã€æ¨¡å‹è¯„æµ‹ã€æŠ€æœ¯æ¶æ„ã€ç®—æ³•çªç ´ç­‰ã€‚

## AI å¼€å‘è€…ç”Ÿæ€

å¼€å‘æ¡†æ¶ã€APIã€SDKã€å¼€æºé¡¹ç›®ã€å¼€å‘è€…å·¥å…·é“¾ç­‰ã€‚

## AI è¡Œä¸šåŠ¨æ€

å…¬å¸æˆ˜ç•¥ã€èèµ„æ”¶è´­ã€äººäº‹å˜åŠ¨ã€æ”¿ç­–æ³•è§„ã€è¡Œä¸šåˆä½œç­‰ã€‚

## AI ç ”ç©¶ä¸è§‚ç‚¹

å­¦æœ¯è®ºæ–‡ã€å®éªŒç»“æœã€è¡Œä¸šè§‚å¯Ÿã€è¶‹åŠ¿åˆ†æç­‰ã€‚"""

        if style == "concise":
            item_format = """æ¯ä¸ªåˆ†ç±»ä¸‹çš„æ¡ç›®æ ¼å¼ï¼š
- [å…·ä½“æ ‡é¢˜](æ¨æ–‡URL)ã€‚ä¸€å¥è¯æ ¸å¿ƒäº‹å®ã€‚

ç¤ºä¾‹ï¼š
- [OpenAI å‘å¸ƒ GPT-5](https://x.com/OpenAI/status/123)ã€‚åŸç”Ÿå¤šæ¨¡æ€æ¨ç†ï¼Œæ€§èƒ½å…¨é¢è¶…è¶Š GPT-4oã€‚"""
            rules = """è§„åˆ™ï¼š
- æ¯æ¡åªç”¨ä¸€å¥è¯ï¼Œä¸è¶…è¿‡ 30 å­—ï¼Œåªä¿ç•™æœ€æ ¸å¿ƒçš„äº‹å®
- æœ‰æ•°æ®å°±å†™æ•°æ®ï¼ˆç”¨æˆ·é‡ã€ä»·æ ¼ã€æ€§èƒ½æŒ‡æ ‡ç­‰ï¼‰
- å¤šæ¡æ¨æ–‡è®²åŒä¸€ä»¶äº‹æ—¶åˆå¹¶ä¸ºä¸€æ¡
- æ¯ä¸ªåˆ†ç±»å†…æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½æ’åˆ—
- å¦‚æœæŸä¸ªåˆ†ç±»ä¸‹æ²¡æœ‰å†…å®¹ï¼Œçœç•¥è¯¥åˆ†ç±»
- ä¸åŠ å‰è¨€æˆ–ç»“å°¾æ€»ç»“æ®µè½"""

        elif style == "advanced":
            item_format = """æ¯ä¸ªåˆ†ç±»ä¸‹çš„æ¡ç›®æ ¼å¼ï¼š
- [å…·ä½“æ ‡é¢˜](æ¨æ–‡URL)ã€‚é™ˆè¿°å¥æè¿°ï¼Œä¿¡æ¯å¯†åº¦é«˜ã€‚
  **ä¸ºä»€ä¹ˆé‡è¦**ï¼šåˆ†æè¿™æ¡ä¿¡æ¯å¯¹ AI è¡Œä¸š/å¼€å‘è€…/ç”¨æˆ·çš„å½±å“å’Œæ„ä¹‰ã€‚

ç¤ºä¾‹ï¼š
- [Anthropic å‘å¸ƒ Claude Opus 4.5 å®‰å…¨é£é™©æŠ¥å‘Š](https://x.com/AnthropicAI/status/123)ã€‚Anthropic å› å…¶ä¸‹ä¸€ä»£æ¨¡å‹æ¥è¿‘ AI Safety Level 4 é˜ˆå€¼ï¼ˆå³å…·å¤‡è‡ªä¸» AI ç ”å‘èƒ½åŠ›ï¼‰ï¼Œä¸»åŠ¨å‘å¸ƒè¯„ä¼°æŠ¥å‘Šã€‚
  **ä¸ºä»€ä¹ˆé‡è¦**ï¼šè¿™æ˜¯é¦–å®¶ä¸ºå•ä¸ªæ¨¡å‹å‘å¸ƒç ´åæ€§é£é™©æŠ¥å‘Šçš„ AI å…¬å¸ï¼Œå¯èƒ½æ¨åŠ¨è¡Œä¸šå»ºç«‹ç±»ä¼¼çš„å®‰å…¨è¯„ä¼°æ ‡å‡†ï¼Œå¯¹ AI å®‰å…¨ç›‘ç®¡èµ°å‘æœ‰é£å‘æ ‡æ„ä¹‰ã€‚"""
            rules = """è§„åˆ™ï¼š
- æ ‡é¢˜å…·ä½“ç²¾ç‚¼ï¼Œæè¿°ç”¨ä¸€åˆ°ä¸¤ä¸ªè‡ªç„¶é™ˆè¿°å¥
- æ¯æ¡å¿…é¡»é™„å¸¦"ä¸ºä»€ä¹ˆé‡è¦"åˆ†æï¼ˆ1-2 å¥è¯ï¼Œèšç„¦å®é™…å½±å“ï¼‰
- æœ‰æ•°æ®å°±å†™æ•°æ®ï¼ˆç”¨æˆ·é‡ã€ä»·æ ¼ã€æ€§èƒ½æŒ‡æ ‡ã€Star æ•°ç­‰ï¼‰
- å¦‚æœæ˜¯å·¥å…·æˆ–äº§å“ï¼šå†™æ˜æ€ä¹ˆè·å–ã€æœ‰ä»€ä¹ˆç‹¬ç‰¹ä¼˜åŠ¿
- å¦‚æœæ˜¯ç ”ç©¶æˆ–æŠ¥å‘Šï¼šå†™æ˜ä¸»è¦å‘ç°å’Œå®é™…æ„ä¹‰
- å¤šæ¡æ¨æ–‡è®²åŒä¸€ä»¶äº‹æ—¶åˆå¹¶ä¸ºä¸€æ¡ï¼Œç»¼åˆæ‰€æœ‰ä¿¡æ¯æº
- æ¯ä¸ªåˆ†ç±»å†…æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½æ’åˆ—
- å¦‚æœæŸä¸ªåˆ†ç±»ä¸‹æ²¡æœ‰å†…å®¹ï¼Œçœç•¥è¯¥åˆ†ç±»
- ä¸åŠ å‰è¨€æˆ–ç»“å°¾æ€»ç»“æ®µè½"""

        else:  # standardï¼ˆé»˜è®¤ï¼Œä¸åŸæœ‰è¡Œä¸ºä¸€è‡´ï¼‰
            item_format = """æ¯ä¸ªåˆ†ç±»ä¸‹çš„æ¡ç›®æ ¼å¼ï¼š
- [å…·ä½“æ ‡é¢˜](æ¨æ–‡URL)ã€‚é™ˆè¿°å¥æè¿°ï¼Œä¿¡æ¯å¯†åº¦é«˜ã€‚

ç¤ºä¾‹ï¼š
- [Anthropic å‘å¸ƒ Claude Opus 4.5 å®‰å…¨é£é™©æŠ¥å‘Š](https://x.com/AnthropicAI/status/123)ã€‚Anthropic å› å…¶ä¸‹ä¸€ä»£æ¨¡å‹æ¥è¿‘ AI Safety Level 4 é˜ˆå€¼ï¼ˆå³å…·å¤‡è‡ªä¸» AI ç ”å‘èƒ½åŠ›ï¼‰ï¼Œä¸»åŠ¨å‘å¸ƒè¯„ä¼°æŠ¥å‘Šï¼Œæ‰¿è¯ºä¸ºæ‰€æœ‰æœªæ¥æ¨¡å‹æ’°å†™ç ´åæ€§é£é™©æŠ¥å‘Šï¼Œè¿™æ˜¯é¦–å®¶ä¸ºå•ä¸ªæ¨¡å‹å‘å¸ƒæ­¤ç±»æ–‡ä»¶çš„ AI å…¬å¸ã€‚"""
            rules = """è§„åˆ™ï¼š
- æ ‡é¢˜å…·ä½“ç²¾ç‚¼ï¼Œæè¿°ç”¨ä¸€åˆ°ä¸¤ä¸ªè‡ªç„¶é™ˆè¿°å¥ï¼ŒæŠŠå…³é”®ä¿¡æ¯ä¸²åœ¨ä¸€èµ·
- æœ‰æ•°æ®å°±å†™æ•°æ®ï¼ˆç”¨æˆ·é‡ã€ä»·æ ¼ã€æ€§èƒ½æŒ‡æ ‡ã€Star æ•°ç­‰ï¼‰
- å¦‚æœæ˜¯å·¥å…·æˆ–äº§å“ï¼šå†™æ˜æ€ä¹ˆè·å–ã€æœ‰ä»€ä¹ˆç‹¬ç‰¹ä¼˜åŠ¿
- å¦‚æœæ˜¯ç ”ç©¶æˆ–æŠ¥å‘Šï¼šå†™æ˜ä¸»è¦å‘ç°å’Œå®é™…æ„ä¹‰
- å¦‚æœæ¨æ–‡å¼•ç”¨/è½¬å‘äº†å…¶ä»–å†…å®¹ï¼Œæè¿°åŸå§‹å†…å®¹
- å¤šæ¡æ¨æ–‡è®²åŒä¸€ä»¶äº‹æ—¶åˆå¹¶ä¸ºä¸€æ¡ï¼Œç»¼åˆæ‰€æœ‰ä¿¡æ¯æº
- æ¯ä¸ªåˆ†ç±»å†…æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½æ’åˆ—
- å¦‚æœæŸä¸ªåˆ†ç±»ä¸‹æ²¡æœ‰å†…å®¹ï¼Œçœç•¥è¯¥åˆ†ç±»
- ä¸åŠ å‰è¨€æˆ–ç»“å°¾æ€»ç»“æ®µè½"""

        category_block = f"è¾“å‡ºç»“æ„ï¼ˆä¸¥æ ¼éµå¾ªï¼‰ï¼š\n\n{categories}\n\n{item_format}"
        rules_block = rules

        # è¿½åŠ ç”¨æˆ·è‡ªå®šä¹‰ prompt
        if custom_prompt:
            rules_block += f"\n\nç”¨æˆ·ç‰¹åˆ«è¦æ±‚ï¼š\n{custom_prompt}"

        return category_block, rules_block

    def generate_ai_summary(self, followings_data, trending_tweets):
        """è°ƒç”¨ Claude API ç”Ÿæˆæ™ºèƒ½æ€»ç»“ï¼Œå¯é€‰åŒæ—¶è¿›è¡Œ AI ç›¸å…³æ€§åˆ¤æ–­

        Returns:
            (summary_text, ai_tweet_ids) - ai_tweet_ids ä¸º set æˆ– None
        """
        summary_config = self.config.get("ai_summary", {})
        if not summary_config.get("enabled", True):
            return None, None

        api_key = (
            summary_config.get("api_key", "")
            or os.environ.get("ANTHROPIC_AUTH_TOKEN", "")
            or os.environ.get("ANTHROPIC_API_KEY", "")
        )
        if not api_key:
            print("  è·³è¿‡ AI æ€»ç»“ï¼ˆæœªé…ç½® Anthropic API Keyï¼‰")
            return None, None

        base_url = (
            summary_config.get("base_url", "")
            or os.environ.get("ANTHROPIC_BASE_URL", "")
            or "https://api.anthropic.com"
        )

        ai_filter = summary_config.get("ai_filter", False)
        style = summary_config.get("style", "standard")
        custom_prompt = summary_config.get("custom_prompt", "")

        # æ„å»ºæ¨æ–‡å†…å®¹
        content_parts = self._build_tweet_lines(followings_data, trending_tweets, with_id=ai_filter)
        all_content = "\n".join(content_parts)

        window_desc = ""
        if self.hours_ago:
            window_desc = f"ï¼ˆæœ¬æ¬¡è¦†ç›–æœ€è¿‘ {self.hours_ago} å°æ—¶ï¼‰"

        # æ ¹æ® style ç”Ÿæˆ prompt
        category_block, rules_block = self._build_style_prompts(style, custom_prompt)

        model = summary_config.get("model", "claude-sonnet-4-5-20250929")
        max_tokens = summary_config.get("max_tokens", 4096)
        max_input_tokens = summary_config.get("max_input_tokens", 150000)
        api_url = f"{base_url.rstrip('/')}/v1/messages"
        headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json",
        }

        if ai_filter:
            return self._filter_and_summarize(
                followings_data, trending_tweets, all_content, window_desc,
                category_block, rules_block, model, max_tokens, max_input_tokens,
                api_url, headers, base_url
            )
        else:
            return self._batched_summarize(
                all_content, window_desc, category_block, rules_block,
                model, max_tokens, max_input_tokens, api_url, headers, base_url
            )

    # â”€â”€ Claude API è¾…åŠ©æ–¹æ³• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _estimate_tokens(text):
        """ç²—ç•¥ä¼°ç®—æ–‡æœ¬ token æ•°ï¼ˆä¸­è‹±æ··åˆçº¦ 0.4 token/charï¼‰"""
        return int(len(text) * 0.4)

    def _call_claude_api(self, prompt, model, max_tokens, api_url, headers,
                         timeout=120, max_retries=3):
        """è°ƒç”¨ Claude APIï¼Œè‡ªåŠ¨é‡è¯• + è¶…æ—¶é€’å¢ï¼Œè¿”å› (response_text, usage_dict)"""
        cur_timeout = timeout
        for attempt in range(1, max_retries + 1):
            try:
                resp = requests.post(
                    api_url, headers=headers,
                    json={
                        "model": model,
                        "max_tokens": max_tokens,
                        "messages": [{"role": "user", "content": prompt}],
                    },
                    timeout=cur_timeout,
                )
                resp.raise_for_status()
                result = resp.json()
                return result["content"][0]["text"], result.get("usage", {})
            except Exception as e:
                if attempt < max_retries:
                    wait = 10 * attempt          # 10s, 20s
                    cur_timeout = int(cur_timeout * 1.5)  # 120â†’180â†’270
                    print(f"    âš  é‡è¯• ({attempt}/{max_retries}, {wait}s å, "
                          f"timeoutâ†’{cur_timeout}s): {type(e).__name__}")
                    time.sleep(wait)
                else:
                    raise

    def _batch_lines_by_tokens(self, lines, max_content_tokens):
        """å°†æ–‡æœ¬è¡Œåˆ—è¡¨æŒ‰ token æ•°åˆ†æ‰¹ï¼Œç¡®ä¿æ¯æ‰¹ä¸è¶…è¿‡ max_content_tokens"""
        batches = []
        current_batch = []
        current_tokens = 0
        for line in lines:
            line_tokens = self._estimate_tokens(line)
            if current_tokens + line_tokens > max_content_tokens and current_batch:
                batches.append(current_batch)
                current_batch = []
                current_tokens = 0
            current_batch.append(line)
            current_tokens += line_tokens
        if current_batch:
            batches.append(current_batch)
        return batches

    @staticmethod
    def _parse_ai_tweet_ids(response_text):
        """ä» Claude å“åº”ä¸­æå– AI æ¨æ–‡ ID é›†åˆ"""
        json_match = re.search(r'```json\s*\n(.*?)\n```', response_text, re.DOTALL)
        if not json_match:
            json_match = re.search(r'\{[^}]*"ai_tweet_ids"[^}]*\}', response_text, re.DOTALL)
            if json_match:
                id_data = json.loads(json_match.group(0))
            else:
                return None
        else:
            id_data = json.loads(json_match.group(1))
        return set(str(i) for i in id_data.get("ai_tweet_ids", []))

    @staticmethod
    def _parse_urgent_ids(response_text):
        """ä» Claude å“åº”ä¸­æå–ç´§æ€¥æ¨æ–‡ ID é›†åˆ"""
        try:
            json_match = re.search(r'```json\s*\n(.*?)\n```', response_text, re.DOTALL)
            if json_match:
                id_data = json.loads(json_match.group(1))
            else:
                json_match = re.search(r'\{[^}]*"urgent_ids"[^}]*\}', response_text, re.DOTALL)
                if json_match:
                    id_data = json.loads(json_match.group(0))
                else:
                    return set()
            return set(str(i) for i in id_data.get("urgent_ids", []))
        except (json.JSONDecodeError, AttributeError):
            return set()

    def _build_tweet_lines(self, followings_data, trending_tweets, with_id=False):
        """æ„å»ºæ¨æ–‡æ–‡æœ¬è¡Œåˆ—è¡¨ï¼Œè¿”å› list of str"""
        lines = []
        if followings_data:
            lines.append("## å…³æ³¨åˆ—è¡¨æ¨æ–‡ï¼š")
            for ud in followings_data:
                uname = ud["user"]["username"]
                for t in ud["tweets"]:
                    tid = t.get("id", "")
                    likes = t.get("likeCount", 0)
                    views = t.get("viewCount", 0)
                    text = t.get("text", "")[:200]
                    url = t.get("url", "")
                    quoted = t.get("quoted_tweet")
                    quoted_info = ""
                    if quoted:
                        q_author = quoted.get("author", {}).get("userName", "?")
                        q_text = quoted.get("text", "")[:150]
                        quoted_info = f"\n  [å¼•ç”¨ @{q_author}]: {q_text}"
                    id_prefix = f"[ID:{tid}] " if with_id else ""
                    lines.append(f"- {id_prefix}@{uname} ({views:,} views, {likes} likes): {text}{quoted_info}\n  URL: {url}")
        if trending_tweets:
            lines.append("\n## å…¨ç½‘çƒ­é—¨ AI æ¨æ–‡ï¼š")
            for t in trending_tweets:
                tid = t.get("id", "")
                author = t.get("author", {}).get("userName", "?")
                likes = t.get("likeCount", 0)
                views = t.get("viewCount", 0)
                text = t.get("text", "")[:200]
                url = t.get("url", "")
                id_prefix = f"[ID:{tid}] " if with_id else ""
                lines.append(f"- {id_prefix}@{author} ({views:,} views, {likes:,} likes): {text}\n  URL: {url}")
        return lines

    def _filter_and_summarize(
        self, followings_data, trending_tweets, all_content, window_desc,
        category_block, rules_block, model, max_tokens, max_input_tokens,
        api_url, headers, base_url
    ):
        """ai_filter æ¨¡å¼ï¼šåˆ†æ‰¹ç­›é€‰ AI æ¨æ–‡ â†’ åˆ†æ‰¹ç”Ÿæˆæ€»ç»“"""
        prompt_overhead = 2000  # ç­›é€‰ prompt æ¨¡æ¿å¼€é”€
        max_content_tokens = max_input_tokens - prompt_overhead

        # â”€â”€ Pass 1: åˆ†æ‰¹ç­›é€‰ â”€â”€
        tweet_lines = self._build_tweet_lines(followings_data, trending_tweets, with_id=True)
        batches = self._batch_lines_by_tokens(tweet_lines, max_content_tokens)
        total_batches = len(batches)

        ai_tweet_ids = set()
        urgent_ids = set()
        total_input = 0
        total_output = 0

        filter_label = f"[Pass 1/2] ç­›é€‰ AI æ¨æ–‡" if total_batches == 1 else f"[Pass 1/2] åˆ†æ‰¹ç­›é€‰ AI æ¨æ–‡ï¼ˆ{total_batches} æ‰¹ï¼‰"
        print(f"  {filter_label} ({model}) via {base_url}...")

        for bi, batch in enumerate(batches, 1):
            if total_batches > 1:
                print(f"    æ‰¹æ¬¡ {bi}/{total_batches}...", end=" ", flush=True)
            ids, urg, inp, out = self._filter_batch_robust(
                batch, window_desc, model, max_tokens, api_url, headers
            )
            total_input += inp
            total_output += out
            ai_tweet_ids.update(ids)
            urgent_ids.update(urg)
            if total_batches > 1:
                print(f"{len(ids)} æ¡")

        urgent_label = f"ï¼ˆğŸ”´ {len(urgent_ids)} æ¡çªå‘ï¼‰" if urgent_ids else ""
        print(f"  ç­›é€‰å®Œæˆï¼ˆ{total_input} + {total_output} tokensï¼‰â†’ {len(ai_tweet_ids)} æ¡ AI ç›¸å…³{urgent_label}")
        self._urgent_ids = urgent_ids  # ä¿å­˜ä¾›åç»­æ¨é€ä½¿ç”¨

        if not ai_tweet_ids:
            print("  è­¦å‘Š: æœªè¯†åˆ«å‡º AI æ¨æ–‡ï¼Œä¿ç•™æ‰€æœ‰æ¨æ–‡")
            return None, None

        # â”€â”€ Pass 2: ç”¨ç­›é€‰åçš„æ¨æ–‡ç”Ÿæˆæ€»ç»“ â”€â”€
        filtered_lines = self._build_filtered_lines(followings_data, trending_tweets, ai_tweet_ids)
        return self._batched_summarize_from_lines(
            filtered_lines, window_desc, category_block, rules_block,
            model, max_tokens, max_input_tokens, api_url, headers, base_url,
            ai_tweet_ids
        )

    def _filter_batch_robust(self, lines, window_desc, model, max_tokens,
                             api_url, headers, depth=0):
        """ç­›é€‰å•ä¸ªæ‰¹æ¬¡ï¼Œå¤±è´¥åè‡ªåŠ¨æ‹†åˆ†ä¸ºå­æ‰¹æ¬¡é‡è¯•ã€‚
        è¿”å› (ai_tweet_ids: set, input_tokens: int, output_tokens: int)"""
        batch_content = "\n".join(lines)
        filter_prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI è¡Œä¸šä¿¡æ¯ç­›é€‰å‘˜ã€‚ä»¥ä¸‹æ˜¯ä» Twitter æŠ“å–çš„æ¨æ–‡åˆ—è¡¨{window_desc}ã€‚

ä»»åŠ¡ï¼š
1. ä»ä¸­æ‰¾å‡ºæ‰€æœ‰ä¸ AI é¢†åŸŸç›¸å…³çš„æ¨æ–‡ï¼ˆåŒ…æ‹¬ AI äº§å“ã€æ¨¡å‹ã€å¼€å‘å·¥å…·ã€è¡Œä¸šåŠ¨æ€ã€ç ”ç©¶ç­‰ï¼‰ã€‚
2. åœ¨ AI ç›¸å…³æ¨æ–‡ä¸­ï¼Œæ ‡å‡ºç´§æ€¥ç¨‹åº¦ä¸º"çªå‘"çš„æ¨æ–‡ï¼ˆé‡å¤§äº§å“å‘å¸ƒã€é‡å¤§æ”¶è´­ã€å®‰å…¨äº‹ä»¶ç­‰éœ€è¦ç«‹å³å…³æ³¨çš„ï¼‰ã€‚

åªè¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼š
```json
{{"ai_tweet_ids": ["id1", "id2", ...], "urgent_ids": ["id3", ...]}}
```

urgent_ids å¿…é¡»æ˜¯ ai_tweet_ids çš„å­é›†ï¼ŒåªåŒ…å«çœŸæ­£é‡å¤§çš„çªå‘äº‹ä»¶ï¼ˆæ¯æ‰¹é€šå¸¸ 0-2 æ¡ï¼‰ã€‚

---
{batch_content}"""
        try:
            resp_text, usage = self._call_claude_api(
                filter_prompt, model, max_tokens, api_url, headers)
            ids = self._parse_ai_tweet_ids(resp_text) or set()
            urgent = self._parse_urgent_ids(resp_text) or set()
            return ids, urgent, usage.get("input_tokens", 0), usage.get("output_tokens", 0)
        except Exception as e:
            # è¡Œæ•°å¤ªå°‘æ— æ³•ç»§ç»­æ‹†åˆ†
            if len(lines) <= 10:
                indent = "  " * (depth + 2)
                print(f"\n    {indent}âœ— å­æ‰¹æ¬¡ä»å¤±è´¥ä¸”æ— æ³•ç»§ç»­æ‹†åˆ†ï¼ˆ{len(lines)} è¡Œï¼‰: {type(e).__name__}")
                return set(), set(), 0, 0
            mid = len(lines) // 2
            indent = "  " * (depth + 2)
            print(f"\n    {indent}â†³ æ‹†åˆ†ä¸º 2 ä¸ªå­æ‰¹æ¬¡é‡è¯•ï¼ˆå„ ~{mid} è¡Œï¼‰...", end=" ", flush=True)
            ids1, urg1, in1, out1 = self._filter_batch_robust(
                lines[:mid], window_desc, model, max_tokens, api_url, headers, depth + 1)
            ids2, urg2, in2, out2 = self._filter_batch_robust(
                lines[mid:], window_desc, model, max_tokens, api_url, headers, depth + 1)
            return ids1 | ids2, urg1 | urg2, in1 + in2, out1 + out2

    def _build_filtered_lines(self, followings_data, trending_tweets, ai_tweet_ids):
        """æ ¹æ® AI æ¨æ–‡ ID æ„å»ºç­›é€‰åçš„å†…å®¹è¡Œ"""
        lines = []
        if followings_data:
            lines.append("## å…³æ³¨åˆ—è¡¨æ¨æ–‡ï¼š")
            for ud in followings_data:
                uname = ud["user"]["username"]
                for t in ud["tweets"]:
                    if str(t.get("id", "")) not in ai_tweet_ids:
                        continue
                    likes = t.get("likeCount", 0)
                    views = t.get("viewCount", 0)
                    text = t.get("text", "")[:200]
                    url = t.get("url", "")
                    quoted = t.get("quoted_tweet")
                    quoted_info = ""
                    if quoted:
                        q_author = quoted.get("author", {}).get("userName", "?")
                        q_text = quoted.get("text", "")[:150]
                        quoted_info = f"\n  [å¼•ç”¨ @{q_author}]: {q_text}"
                    lines.append(f"- @{uname} ({views:,} views, {likes} likes): {text}{quoted_info}\n  URL: {url}")
        if trending_tweets:
            lines.append("\n## å…¨ç½‘çƒ­é—¨ AI æ¨æ–‡ï¼š")
            for t in trending_tweets:
                if str(t.get("id", "")) not in ai_tweet_ids:
                    continue
                author = t.get("author", {}).get("userName", "?")
                likes = t.get("likeCount", 0)
                views = t.get("viewCount", 0)
                text = t.get("text", "")[:200]
                url = t.get("url", "")
                lines.append(f"- @{author} ({views:,} views, {likes:,} likes): {text}\n  URL: {url}")
        return lines

    def _batched_summarize(
        self, all_content, window_desc, category_block, rules_block,
        model, max_tokens, max_input_tokens, api_url, headers, base_url
    ):
        """é ai_filter æ¨¡å¼ï¼šæŒ‰ token åˆ†æ‰¹ç”Ÿæˆæ€»ç»“"""
        content_lines = all_content.split("\n")
        return self._batched_summarize_from_lines(
            content_lines, window_desc, category_block, rules_block,
            model, max_tokens, max_input_tokens, api_url, headers, base_url,
            ai_tweet_ids=None
        )

    def _batched_summarize_from_lines(
        self, content_lines, window_desc, category_block, rules_block,
        model, max_tokens, max_input_tokens, api_url, headers, base_url,
        ai_tweet_ids
    ):
        """é€šç”¨åˆ†æ‰¹æ€»ç»“ï¼šæ”¯æŒ ai_filter å’Œé ai_filter æ¨¡å¼"""
        prompt_overhead = 3000  # æ€»ç»“ prompt æ¨¡æ¿å¼€é”€
        max_content_tokens = max_input_tokens - prompt_overhead

        batches = self._batch_lines_by_tokens(content_lines, max_content_tokens)
        total_batches = len(batches)

        summary_label = "[Pass 2/2] ç”Ÿæˆæ€»ç»“" if ai_tweet_ids is not None else "ç”Ÿæˆæ€»ç»“"
        if total_batches > 1:
            summary_label += f"ï¼ˆ{total_batches} æ‰¹ï¼‰"
        print(f"  {summary_label} ({model}) via {base_url}...")

        partial_summaries = []
        total_input = 0
        total_output = 0

        for bi, batch in enumerate(batches, 1):
            batch_label = f"ï¼ˆç¬¬ {bi}/{total_batches} æ‰¹ï¼‰" if total_batches > 1 else ""
            if total_batches > 1:
                print(f"    æ‰¹æ¬¡ {bi}/{total_batches}...", end=" ", flush=True)

            summaries, inp, out = self._summarize_batch_robust(
                batch, window_desc, batch_label,
                category_block, rules_block, model, max_tokens, api_url, headers
            )
            total_input += inp
            total_output += out
            partial_summaries.extend(summaries)
            if total_batches > 1 and summaries:
                print(f"å®Œæˆï¼ˆ{inp} + {out} tokensï¼‰")

        if not partial_summaries:
            print("  æ€»ç»“å¤±è´¥: æ‰€æœ‰æ‰¹æ¬¡å‡å¤±è´¥")
            return None, ai_tweet_ids

        # å•ä»½æ€»ç»“ â†’ ç›´æ¥è¿›å…¥æ ¡éªŒ
        if len(partial_summaries) == 1:
            print(f"  æ€»ç»“å®Œæˆï¼ˆ{total_input} + {total_output} tokensï¼‰")
            final = self._validate_summary(
                partial_summaries[0], model, max_tokens, api_url, headers)
            return final, ai_tweet_ids

        # å¤šä»½æ€»ç»“ â†’ åˆå¹¶ + æ ¡éªŒï¼ˆåˆå¹¶ prompt æœ¬èº«åŒ…å«å»é‡ï¼Œå› æ­¤åˆå¹¶å³æ ¡éªŒï¼‰
        print(f"  æ‰¹æ¬¡æ€»ç»“å®Œæˆï¼ˆ{total_input} + {total_output} tokensï¼‰ï¼Œåˆå¹¶æ ¡éªŒä¸­...")
        merge_content = "\n\n---\n\n".join(
            f"### æ‰¹æ¬¡ {i+1} æ€»ç»“ï¼š\n{s}" for i, s in enumerate(partial_summaries)
        )
        merge_prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI è¡Œä¸šä¿¡æ¯è´¨é‡å®¡æ ¸å‘˜ã€‚ä»¥ä¸‹æ˜¯åˆ†æ‰¹å¤„ç†äº§ç”Ÿçš„å¤šä»½ AI æ¨æ–‡æ€»ç»“ï¼Œè¯·åˆå¹¶ä¸ºä¸€ä»½é«˜ç½®ä¿¡åº¦çš„æœ€ç»ˆæ—¥æŠ¥ã€‚

ä»»åŠ¡ï¼š
1. åˆå¹¶æŠ¥é“åŒä¸€äº‹ä»¶çš„ä¸åŒæ¡ç›®ï¼ˆä¿ç•™æœ€å®Œæ•´çš„æè¿°ï¼Œç»¼åˆå¤šä¸ªä¿¡æ¯æºï¼‰
2. å»é™¤å®Œå…¨é‡å¤çš„æ¡ç›®
3. æ£€æŸ¥é«˜åº¦ç›¸ä¼¼çš„æ¡ç›®ï¼ˆåŒä¸€äº§å“/äº‹ä»¶çš„ä¸åŒè§’åº¦ï¼‰ï¼Œåˆå¹¶ä¸ºä¸€æ¡ç»¼åˆæè¿°
4. ç¡®ä¿æ¯æ¡éƒ½ä¿ç•™äº†æœ‰æ•ˆçš„æ¨æ–‡ URLï¼ˆæ ¼å¼ï¼š[æ ‡é¢˜](URL)ï¼‰
5. ç¡®ä¿åˆ†ç±»å‡†ç¡®ï¼Œæ¯ä¸ªåˆ†ç±»å†…æŒ‰é‡è¦æ€§æ’åˆ—
6. ä¿æŒæ ¼å¼ä¸€è‡´ï¼š- [æ ‡é¢˜](URL)ã€‚æè¿°ã€‚

è§„åˆ™ï¼š
- ä¿æŒåŸæœ‰çš„åˆ†ç±»ç»“æ„ï¼ˆæœ¬æœŸè¦ç‚¹ã€AI äº§å“ä¸å·¥å…·ã€AI æ¨¡å‹ä¸æŠ€æœ¯ç­‰ï¼‰
- å¦‚æœæŸä¸ªåˆ†ç±»ä¸‹æ²¡æœ‰å†…å®¹ï¼Œçœç•¥è¯¥åˆ†ç±»
- ä¸è¦åŠ å‰è¨€æˆ–ç»“å°¾æ€»ç»“æ®µè½
- ç›´æ¥è¾“å‡ºæœ€ç»ˆç‰ˆæœ¬

{merge_content}"""

        merged_text, merge_usage = self._call_claude_api(
            merge_prompt, model, max_tokens, api_url, headers)
        total_input += merge_usage.get("input_tokens", 0)
        total_output += merge_usage.get("output_tokens", 0)
        print(f"  åˆå¹¶æ ¡éªŒå®Œæˆï¼ˆæ€»è®¡ {total_input} + {total_output} tokensï¼‰")
        return merged_text, ai_tweet_ids

    def _summarize_batch_robust(self, lines, window_desc, batch_label,
                                category_block, rules_block,
                                model, max_tokens, api_url, headers, depth=0):
        """æ€»ç»“å•ä¸ªæ‰¹æ¬¡ï¼Œå¤±è´¥åè‡ªåŠ¨æ‹†åˆ†ä¸ºå­æ‰¹æ¬¡é‡è¯•ã€‚
        è¿”å› (summaries: list[str], input_tokens: int, output_tokens: int)"""
        batch_content = "\n".join(lines)
        prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI è¡Œä¸šä¿¡æ¯æ•´ç†å‘˜ã€‚ä»¥ä¸‹æ˜¯ä» Twitter æŠ“å–çš„ AI ç›¸å…³æ¨æ–‡{window_desc}{batch_label}ã€‚

ä»»åŠ¡ï¼šç”Ÿæˆç»“æ„åŒ–çš„åˆ†ç±»æ—¥æŠ¥ã€‚

è¯»è€…ç”»åƒï¼šå…³æ³¨ AI å‰æ²¿åŠ¨æ€çš„ä»ä¸šè€…ã€‚ä»–ä»¬æƒ³ä»æ¯æ¡æ–°é—»ä¸­å¿«é€Ÿäº†è§£ï¼šå‘ç”Ÿäº†ä»€ä¹ˆã€è°å‘å¸ƒçš„ã€æ ¸å¿ƒå†…å®¹ï¼ˆåŠŸèƒ½/æŒ‡æ ‡/æ•°æ®/ä»·æ ¼ï¼‰ã€æ€ä¹ˆè·å–æˆ–æœ‰ä»€ä¹ˆæ„ä¹‰ã€‚

{category_block}

{rules_block}

---
{batch_content}"""

        try:
            resp_text, usage = self._call_claude_api(
                prompt, model, max_tokens, api_url, headers)
            return [resp_text], usage.get("input_tokens", 0), usage.get("output_tokens", 0)
        except Exception as e:
            if len(lines) <= 10:
                indent = "  " * (depth + 2)
                print(f"\n    {indent}âœ— å­æ‰¹æ¬¡ä»å¤±è´¥ä¸”æ— æ³•ç»§ç»­æ‹†åˆ†ï¼ˆ{len(lines)} è¡Œï¼‰: {type(e).__name__}")
                return [], 0, 0
            mid = len(lines) // 2
            indent = "  " * (depth + 2)
            print(f"\n    {indent}â†³ æ‹†åˆ†ä¸º 2 ä¸ªå­æ‰¹æ¬¡é‡è¯•ï¼ˆå„ ~{mid} è¡Œï¼‰...", end=" ", flush=True)
            s1, in1, out1 = self._summarize_batch_robust(
                lines[:mid], window_desc, batch_label,
                category_block, rules_block, model, max_tokens, api_url, headers, depth + 1)
            s2, in2, out2 = self._summarize_batch_robust(
                lines[mid:], window_desc, batch_label,
                category_block, rules_block, model, max_tokens, api_url, headers, depth + 1)
            return s1 + s2, in1 + in2, out1 + out2

    def _validate_summary(self, summary_text, model, max_tokens, api_url, headers):
        """æœ€ç»ˆæ ¡éªŒï¼šå»é‡ã€åˆå¹¶ç›¸ä¼¼æ¡ç›®ã€æ ¼å¼ä¸€è‡´æ€§æ£€æŸ¥ã€‚
        è¿”å›æ ¡éªŒåçš„æ€»ç»“æ–‡æœ¬ã€‚"""
        print("  [æ ¡éªŒ] æœ€ç»ˆå»é‡ä¸åˆå¹¶æ£€æŸ¥...", end=" ", flush=True)
        validate_prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI ä¿¡æ¯è´¨é‡å®¡æ ¸å‘˜ã€‚ä»¥ä¸‹æ˜¯ä¸€ä»½ AI æ¨æ–‡æ—¥æŠ¥ï¼Œè¯·è¿›è¡Œæœ€ç»ˆæ ¡éªŒå’Œä¼˜åŒ–ã€‚

æ ¡éªŒä»»åŠ¡ï¼š
1. é‡å¤æ£€æµ‹ï¼šæ‰¾å‡ºæè¿°åŒä¸€äº‹ä»¶/æ–°é—»çš„é‡å¤æ¡ç›®ï¼Œåªä¿ç•™ä¿¡æ¯æœ€å®Œæ•´çš„ä¸€æ¡
2. ç›¸ä¼¼åˆå¹¶ï¼šå°†é«˜åº¦ç›¸å…³çš„æ¡ç›®ï¼ˆåŒä¸€äº§å“/äº‹ä»¶çš„ä¸åŒè§’åº¦ï¼‰åˆå¹¶ä¸ºä¸€æ¡ç»¼åˆæè¿°
3. URL æ£€æŸ¥ï¼šç¡®ä¿æ¯æ¡éƒ½åŒ…å«æœ‰æ•ˆçš„æ¨æ–‡é“¾æ¥ï¼Œæ ¼å¼ä¸º [æ ‡é¢˜](URL)
4. åˆ†ç±»æ ¡éªŒï¼šç¡®è®¤æ¡ç›®æ”¾åœ¨äº†æ­£ç¡®çš„åˆ†ç±»ä¸‹ï¼Œå¦‚æœ‰é”™è¯¯åˆ™è°ƒæ•´
5. æ ¼å¼ç»Ÿä¸€ï¼šæ¯æ¡æ ¼å¼ä¸º - [æ ‡é¢˜](URL)ã€‚æè¿°ã€‚

è§„åˆ™ï¼š
- å¦‚æœå†…å®¹å·²ç»å¾ˆå¥½ï¼Œæ— éœ€é‡å¤/åˆå¹¶é—®é¢˜ï¼Œç›´æ¥åŸæ ·è¾“å‡º
- å¦‚æœæœ‰éœ€è¦ä¿®æ­£çš„ï¼Œè¾“å‡ºä¿®æ­£åçš„å®Œæ•´ç‰ˆæœ¬
- ä¿æŒåŸæœ‰åˆ†ç±»ç»“æ„
- ä¸è¦åŠ ä»»ä½•è¯´æ˜ã€æ³¨é‡Šã€å‰è¨€æˆ–ç»“å°¾
- ç›´æ¥è¾“å‡ºæœ€ç»ˆç‰ˆæœ¬

---
{summary_text}"""

        try:
            validated, usage = self._call_claude_api(
                validate_prompt, model, max_tokens, api_url, headers)
            print(f"å®Œæˆï¼ˆ{usage.get('input_tokens', 0)} + {usage.get('output_tokens', 0)} tokensï¼‰")
            return validated
        except Exception as e:
            print(f"è·³è¿‡ï¼ˆ{type(e).__name__}ï¼‰ï¼Œä½¿ç”¨åŸå§‹æ€»ç»“")
            return summary_text

    # â”€â”€ é€šçŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def send_notification(self, title, message):
        if not self.notifications_config.get("enabled", True):
            return
        sound = self.notifications_config.get("sound", "Glass")
        try:
            subprocess.run(
                [
                    "osascript", "-e",
                    f'display notification "{message}" with title "{title}" sound name "{sound}"',
                ],
                check=True, capture_output=True,
            )
        except Exception as e:
            print(f"  é€šçŸ¥å¤±è´¥: {e}")

    # â”€â”€ Layer 1: æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run_scrape(self):
        """Layer 1: çº¯æ•°æ®æŠ“å–ï¼Œä¿å­˜åŸå§‹æ¨æ–‡ï¼ˆä¸åšå…³é”®è¯/AI è¿‡æ»¤ï¼‰"""
        username = self.twitter_config["username"]
        now = self.now()
        print(f"=== Twitter Watchdog â€” æŠ“å– ===")
        print(f"ç›‘æ§è´¦æˆ·: @{username}")
        print(f"æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')} (UTC+8)")
        if self.hours_ago:
            print(f"åˆ†é¡µæ·±åº¦: æœ€è¿‘ {self.hours_ago} å°æ—¶")
        print()

        # æ­¥éª¤1: è·å–å…³æ³¨åˆ—è¡¨ + è‡ªå®šä¹‰è´¦å·
        print("[1/3] è·å–å…³æ³¨åˆ—è¡¨...")
        followings = self.get_following()
        print(f"  å…± {len(followings)} ä¸ªå…³æ³¨è´¦æˆ·")

        # åˆå¹¶ custom_accountsï¼ˆä¸åœ¨å…³æ³¨åˆ—è¡¨ä¸­çš„é¢å¤–è´¦å·ï¼‰
        custom_accounts = self.twitter_config.get("custom_accounts", [])
        if custom_accounts:
            existing_usernames = {u.get("username", "").lower() for u in followings}
            added = 0
            for acct in custom_accounts:
                if acct.lower() not in existing_usernames:
                    followings.append({
                        "username": acct,
                        "name": acct,
                        "description": "",
                        "public_metrics": {"followers_count": 0},
                        "_custom": True,
                    })
                    added += 1
            if added:
                print(f"  + {added} ä¸ªè‡ªå®šä¹‰è´¦å·")

        # æ­¥éª¤2: æŠ“å–æ¨æ–‡ï¼ˆå…¨é‡ï¼Œä¸åšå…³é”®è¯/AI è¿‡æ»¤ï¼‰
        print(f"\n[2/3] æŠ“å–æ¨æ–‡ï¼ˆtwitterapi.ioï¼‰...")
        all_data = []
        total_tweets = 0
        api_calls = 0

        for i, user in enumerate(followings, 1):
            uname = user["username"]
            name = user["name"]
            print(f"  [{i}/{len(followings)}] @{uname} ({name})...", end=" ", flush=True)

            try:
                tweets, calls = self.get_tweets(uname)
                api_calls += calls

                new_tweets = []
                for tweet in tweets:
                    tweet_id = tweet.get("id", "")
                    if self.advanced_config.get("deduplicate", True) and self.is_tweet_seen(tweet_id):
                        continue
                    new_tweets.append(tweet)
                    self.mark_tweet_seen(tweet_id)
                    total_tweets += 1

                print(f"{len(new_tweets)} æ¡")

                if new_tweets:
                    all_data.append({"user": user, "tweets": new_tweets})

            except Exception as e:
                print(f"é”™è¯¯: {e}")

        # æ­¥éª¤3: å…¨ç½‘çƒ­é—¨ AI æœç´¢
        trending_config = self.config.get("trending_search", {})
        trending_tweets = []
        if trending_config.get("enabled", True):
            max_trending = trending_config.get("max_tweets", 20)
            print(f"\n[3/3] æœç´¢å…¨ç½‘çƒ­é—¨ AI æ¨æ–‡...")
            try:
                trending_tweets = self.search_trending_ai(max_tweets=max_trending)
                api_calls += 1
                # å»é‡ï¼ˆä¸åšæ—¶é—´çª—å£è¿‡æ»¤ï¼Œä¿ç•™å…¨é‡ï¼‰
                seen_ids = {t.get("id") for ud in all_data for t in ud["tweets"]}
                trending_tweets = [t for t in trending_tweets if t.get("id") not in seen_ids]
                print(f"  æ‰¾åˆ° {len(trending_tweets)} æ¡çƒ­é—¨ AI æ¨æ–‡")
            except Exception as e:
                print(f"  æœç´¢å¤±è´¥: {e}")
        else:
            print(f"\n[3/3] çƒ­é—¨æœç´¢å·²ç¦ç”¨")

        self.save_state()

        # ä¿å­˜åŸå§‹æ•°æ®
        output_path = Path(self.output_config["directory"])
        raw_dir = output_path / "raw"
        raw_dir.mkdir(parents=True, exist_ok=True)

        timestamp = now.strftime("%Y%m%d_%H%M%S")
        raw_file = raw_dir / f"{timestamp}.json"

        raw_data = {
            "metadata": {
                "scraped_at": now.isoformat(),
                "username": username,
                "hours_ago": self.hours_ago,
                "followings_count": len(followings),
                "total_tweets": total_tweets + len(trending_tweets),
                "api_calls": api_calls,
            },
            "followings": all_data,
            "trending": trending_tweets,
        }

        with open(raw_file, "w", encoding="utf-8") as f:
            json.dump(raw_data, f, ensure_ascii=False, indent=2, default=str)

        print(f"\n  åŸå§‹æ•°æ®: {raw_file}")
        print(f"  å…³æ³¨æ¨æ–‡: {total_tweets} æ¡ | çƒ­é—¨æ¨æ–‡: {len(trending_tweets)} æ¡ | API è°ƒç”¨: {api_calls} æ¬¡")

        return str(raw_file)

    # â”€â”€ Layer 2: åˆ†æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _find_raw_files(self, source=None, time_from=None, time_to=None):
        """å®šä½ raw JSON æ–‡ä»¶"""
        output_path = Path(self.output_config["directory"])
        raw_dir = output_path / "raw"

        if source:
            source_path = Path(source)
            if not source_path.is_absolute():
                source_path = output_path / source
            if not source_path.exists():
                print(f"  é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {source_path}")
                return []
            return [source_path]

        if not raw_dir.exists():
            print(f"  é”™è¯¯: raw ç›®å½•ä¸å­˜åœ¨ {raw_dir}")
            return []

        raw_files = sorted(raw_dir.glob("*.json"))
        if not raw_files:
            print(f"  é”™è¯¯: raw ç›®å½•ä¸­æ²¡æœ‰ JSON æ–‡ä»¶")
            return []

        if time_from or time_to:
            matched = []
            for f in raw_files:
                try:
                    file_dt = datetime.strptime(f.stem, "%Y%m%d_%H%M%S").replace(tzinfo=TZ_CN)
                    if time_from and file_dt < time_from:
                        continue
                    if time_to and file_dt > time_to:
                        continue
                    matched.append(f)
                except ValueError:
                    continue
            return matched

        # é»˜è®¤è¿”å›æœ€æ–°çš„ raw æ–‡ä»¶
        return [raw_files[-1]]

    def run_analyze(self, source=None, time_from=None, time_to=None):
        """Layer 2: AI åˆ†æåŸå§‹æ•°æ®"""
        now = self.now()
        print(f"=== Twitter Watchdog â€” åˆ†æ ===")
        print(f"æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')} (UTC+8)")
        print()

        # æ­¥éª¤1: å®šä½ raw æ–‡ä»¶
        print("[1/3] å®šä½åŸå§‹æ•°æ®...")
        raw_files = self._find_raw_files(source=source, time_from=time_from, time_to=time_to)
        if not raw_files:
            print("  æœªæ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶")
            return None

        print(f"  æ‰¾åˆ° {len(raw_files)} ä¸ªæ–‡ä»¶: {', '.join(f.name for f in raw_files)}")

        # åˆå¹¶å¤šä¸ª raw æ–‡ä»¶çš„æ•°æ®
        all_followings = []
        all_trending = []
        source_filenames = []
        for rf in raw_files:
            with open(rf, "r", encoding="utf-8") as f:
                raw_data = json.load(f)
            all_followings.extend(raw_data.get("followings", []))
            all_trending.extend(raw_data.get("trending", []))
            source_filenames.append(rf.name)

        # æ­¥éª¤2: æ—¶é—´çª—å£è¿‡æ»¤ + å…³é”®è¯é¢„è¿‡æ»¤
        print(f"\n[2/3] è¿‡æ»¤æ¨æ–‡...")
        ai_filter = self.config.get("ai_summary", {}).get("ai_filter", False)

        # æ—¶é—´çª—å£
        window_from = time_from
        window_to = time_to
        if self.hours_ago and not time_from and not time_to:
            window_to = now
            window_from = now - timedelta(hours=self.hours_ago)

        filtered_followings = []
        total_before = 0
        total_after_window = 0
        total_after_filter = 0

        for ud in all_followings:
            total_before += len(ud["tweets"])

            # æ—¶é—´çª—å£è¿‡æ»¤
            window_tweets = []
            for t in ud["tweets"]:
                if window_from or window_to:
                    created = self.parse_tweet_time(t.get("createdAt", ""))
                    if created:
                        if window_from and created < window_from:
                            continue
                        if window_to and created > window_to:
                            continue
                window_tweets.append(t)

            total_after_window += len(window_tweets)

            # å…³é”®è¯é¢„è¿‡æ»¤ï¼ˆå½“ ai_filter=true æ—¶è·³è¿‡ï¼Œäº¤ç»™ Claudeï¼‰
            if not ai_filter:
                keyword_tweets = []
                for t in window_tweets:
                    passed, reason = self.filter_tweet(t)
                    if passed:
                        keyword_tweets.append(t)
                total_after_filter += len(keyword_tweets)
                if keyword_tweets:
                    filtered_followings.append({"user": ud["user"], "tweets": keyword_tweets})
            else:
                total_after_filter += len(window_tweets)
                if window_tweets:
                    filtered_followings.append({"user": ud["user"], "tweets": window_tweets})

        # trending ä¹Ÿåšæ—¶é—´çª—å£ + å…³é”®è¯è¿‡æ»¤
        filtered_trending = []
        for t in all_trending:
            if window_from or window_to:
                created = self.parse_tweet_time(t.get("createdAt", ""))
                if created:
                    if window_from and created < window_from:
                        continue
                    if window_to and created > window_to:
                        continue
            if not ai_filter:
                passed, reason = self.filter_tweet(t)
                if not passed:
                    continue
            filtered_trending.append(t)

        time_desc = ""
        if window_from and window_to:
            time_desc = f" ({window_from.strftime('%m/%d %H:%M')} ~ {window_to.strftime('%m/%d %H:%M')})"
        print(f"  åŸå§‹: {total_before} æ¡ â†’ æ—¶é—´çª—å£: {total_after_window} æ¡ â†’ è¿‡æ»¤å: {total_after_filter} æ¡{time_desc}")
        print(f"  çƒ­é—¨: {len(all_trending)} æ¡ â†’ {len(filtered_trending)} æ¡")

        if not filtered_followings and not filtered_trending:
            print("  è¿‡æ»¤åæ— æ¨æ–‡ï¼Œè·³è¿‡åˆ†æ")
            return None

        # æ”¶é›†å›¾ç‰‡ä¿¡æ¯
        for ud in filtered_followings:
            for t in ud["tweets"]:
                self.collect_tweet_image(t)
        for t in filtered_trending:
            self.collect_tweet_image(t)

        # æ­¥éª¤3: AI åˆ†æ
        print(f"\n[3/3] AI åˆ†æ...")
        ai_summary, ai_tweet_ids = self.generate_ai_summary(filtered_followings, filtered_trending)

        # AI ç­›é€‰åè¿‡æ»¤
        final_followings = filtered_followings
        final_trending = filtered_trending
        if ai_tweet_ids is not None:
            final_followings = []
            filtered_count = 0
            for ud in filtered_followings:
                ft = [t for t in ud["tweets"] if str(t.get("id", "")) in ai_tweet_ids]
                if ft:
                    final_followings.append({"user": ud["user"], "tweets": ft})
                    filtered_count += len(ft)
            final_trending = [t for t in filtered_trending if str(t.get("id", "")) in ai_tweet_ids]
            print(f"  AI ç­›é€‰: {filtered_count} æ¡å…³æ³¨ + {len(final_trending)} æ¡çƒ­é—¨")

        # ä¿å­˜åˆ†æç»“æœ
        output_path = Path(self.output_config["directory"])
        analysis_dir = output_path / "analysis"
        analysis_dir.mkdir(parents=True, exist_ok=True)

        timestamp = now.strftime("%Y%m%d_%H%M%S")
        analysis_file = analysis_dir / f"{timestamp}.json"

        total_filtered = sum(len(ud["tweets"]) for ud in final_followings) + len(final_trending)
        analysis_data = {
            "metadata": {
                "analyzed_at": now.isoformat(),
                "source_files": source_filenames,
                "time_window": {
                    "from": window_from.isoformat() if window_from else None,
                    "to": window_to.isoformat() if window_to else None,
                },
                "total_tweets": total_before + len(all_trending),
                "filtered_count": total_filtered,
                "model": self.config.get("ai_summary", {}).get("model", "claude-sonnet-4-5-20250929"),
            },
            "ai_tweet_ids": list(ai_tweet_ids) if ai_tweet_ids else [],
            "urgent_ids": list(getattr(self, '_urgent_ids', set())),
            "summary": ai_summary,
            "filtered_followings": final_followings,
            "filtered_trending": final_trending,
        }

        with open(analysis_file, "w", encoding="utf-8") as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)

        print(f"\n  åˆ†æç»“æœ: {analysis_file}")
        print(f"  ç­›é€‰: {total_before + len(all_trending)} æ¡ â†’ {total_filtered} æ¡ AI ç›¸å…³")

        return str(analysis_file)

    # â”€â”€ Layer 3: æŠ¥å‘Š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _find_analysis_files(self, source=None, daily=None, weekly=None, monthly=None):
        """å®šä½ analysis JSON æ–‡ä»¶"""
        output_path = Path(self.output_config["directory"])
        analysis_dir = output_path / "analysis"

        if source:
            source_path = Path(source)
            if not source_path.is_absolute():
                source_path = output_path / source
            if not source_path.exists():
                print(f"  é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {source_path}")
                return []
            return [source_path]

        if not analysis_dir.exists():
            print(f"  é”™è¯¯: analysis ç›®å½•ä¸å­˜åœ¨ {analysis_dir}")
            return []

        analysis_files = sorted(analysis_dir.glob("*.json"))
        if not analysis_files:
            print(f"  é”™è¯¯: analysis ç›®å½•ä¸­æ²¡æœ‰ JSON æ–‡ä»¶")
            return []

        if daily:
            date_prefix = daily.replace("-", "")
            return [f for f in analysis_files if f.stem.startswith(date_prefix)]

        if weekly:
            start_date = datetime.strptime(weekly, "%Y-%m-%d")
            end_date = start_date + timedelta(days=7)
            matched = []
            for f in analysis_files:
                try:
                    file_dt = datetime.strptime(f.stem[:8], "%Y%m%d")
                    if start_date <= file_dt < end_date:
                        matched.append(f)
                except ValueError:
                    continue
            return matched

        if monthly:
            month_prefix = monthly.replace("-", "")
            return [f for f in analysis_files if f.stem.startswith(month_prefix)]

        # é»˜è®¤è¿”å›æœ€æ–°çš„ analysis æ–‡ä»¶
        return [analysis_files[-1]]

    def run_report(self, source=None, daily=None, weekly=None, monthly=None):
        """Layer 3: ä»åˆ†æç»“æœç”ŸæˆæŠ¥å‘Š"""
        now = self.now()
        print(f"=== Twitter Watchdog â€” æŠ¥å‘Š ===")
        print(f"æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')} (UTC+8)")
        print()

        # æ­¥éª¤1: å®šä½ analysis æ–‡ä»¶
        is_periodic = daily or weekly or monthly
        report_type = "æ—¥æŠ¥" if daily else "å‘¨æŠ¥" if weekly else "æœˆæŠ¥" if monthly else "æŠ¥å‘Š"
        print(f"[1/3] å®šä½åˆ†ææ•°æ®ï¼ˆ{report_type}ï¼‰...")

        analysis_files = self._find_analysis_files(
            source=source, daily=daily, weekly=weekly, monthly=monthly
        )
        if not analysis_files:
            print("  æœªæ‰¾åˆ°åˆ†ææ•°æ®æ–‡ä»¶")
            return

        print(f"  æ‰¾åˆ° {len(analysis_files)} ä¸ªæ–‡ä»¶")

        # è¯»å–åˆ†ææ•°æ®
        all_summaries = []
        all_followings = []
        all_trending = []
        for af in analysis_files:
            with open(af, "r", encoding="utf-8") as f:
                data = json.load(f)
            summary = data.get("summary", "")
            if summary:
                all_summaries.append(summary)
            all_followings.extend(data.get("filtered_followings", []))
            all_trending.extend(data.get("filtered_trending", []))

        if not all_summaries:
            print("  åˆ†ææ•°æ®ä¸­æ— æ€»ç»“å†…å®¹")
            return

        # æ­¥éª¤2: èšåˆï¼ˆå¤šæ–‡ä»¶æ—¶éœ€è¦å»é‡+åˆå¹¶ï¼‰
        print(f"\n[2/3] å¤„ç†æ€»ç»“å†…å®¹...")
        if len(all_summaries) == 1 and not is_periodic:
            final_summary = all_summaries[0]
            print(f"  å•ä»½åˆ†æï¼Œç›´æ¥ä½¿ç”¨")
        else:
            # èšåˆå¤šä»½åˆ†æ
            all_items = []
            for s in all_summaries:
                items = self._parse_summary_items(s)
                all_items.extend(items)
            print(f"  æå– {len(all_items)} æ¡ï¼ˆå»é‡å‰ï¼‰")

            unique_items = self._deduplicate_items(all_items)
            print(f"  å»é‡å: {len(unique_items)} æ¡")

            if is_periodic and unique_items:
                period = "monthly" if monthly else "weekly"
                if monthly:
                    year, month_str = monthly.split("-")
                    period_label = f"{year} å¹´ {int(month_str)} æœˆ"
                elif weekly:
                    start_d = datetime.strptime(weekly, "%Y-%m-%d")
                    end_d = start_d + timedelta(days=7)
                    period_label = f"{start_d.strftime('%m/%d')} ~ {end_d.strftime('%m/%d')}"
                else:
                    period_label = daily
                consolidated = self._claude_consolidate(unique_items, period, period_label)
                final_summary = consolidated or "\n\n".join(item["full_text"] for item in unique_items)
            else:
                # å¤šä»½éå‘¨æœŸæŠ¥å‘Šï¼šç›´æ¥åˆå¹¶
                final_summary = "\n\n".join(all_summaries)

        # æ”¶é›†å›¾ç‰‡ä¿¡æ¯
        for ud in all_followings:
            for t in ud["tweets"]:
                self.collect_tweet_image(t)
        for t in all_trending:
            self.collect_tweet_image(t)

        # æ­¥éª¤3: ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        print(f"\n[3/3] ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶...")
        output_path = Path(self.output_config["directory"])
        reports_dir = output_path / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)

        # ä¸‹è½½å›¾ç‰‡
        ai_summary_with_images = final_summary
        downloaded = self.download_report_images(final_summary, reports_dir)
        if downloaded:
            ai_summary_with_images = self.insert_images_into_summary(final_summary, downloaded)

        timestamp = now.strftime("%Y%m%d_%H%M%S")

        # ç¡®å®šæ–‡ä»¶å
        if daily:
            base_name = f"daily_{daily.replace('-', '')}"
        elif weekly:
            base_name = f"weekly_{weekly.replace('-', '')}"
        elif monthly:
            base_name = f"monthly_{monthly.replace('-', '')}"
        else:
            base_name = timestamp

        # HTML æŠ¥å‘Š
        html_file = reports_dir / f"{base_name}.html"
        self.save_as_html(html_file, ai_summary_with_images, timestamp)
        print(f"  HTML: {html_file}")

        # æ›´æ–° latest.html
        latest_html = reports_dir / "latest.html"
        self.save_as_html(latest_html, ai_summary_with_images, timestamp)

        # Markdown æŠ¥å‘Š
        md_file = reports_dir / f"{base_name}.md"
        self._save_report_markdown(md_file, final_summary, all_followings, all_trending, report_type)
        print(f"  Markdown: {md_file}")

        # é€šçŸ¥
        total = sum(len(ud["tweets"]) for ud in all_followings) + len(all_trending)
        if total > 0:
            threshold = self.notifications_config.get("threshold", 1)
            if self.notifications_config.get("on_new_tweets", True) and total >= threshold:
                following_count = sum(len(ud["tweets"]) for ud in all_followings)
                self.send_notification(
                    "Twitter Watchdog",
                    f"å‘ç° {following_count} æ¡å…³æ³¨ + {len(all_trending)} æ¡çƒ­é—¨ AI æ¨æ–‡ï¼",
                )

        return str(html_file)

    def _save_report_markdown(self, output_file, summary, followings_data, trending_tweets, report_type="æŠ¥å‘Š"):
        """ä»åˆ†ææ•°æ®ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        now = self.now()
        ts = now.strftime("%Y-%m-%d %H:%M:%S")
        following_count = sum(len(ud["tweets"]) for ud in followings_data)
        total = following_count + len(trending_tweets)

        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"# AI æ¨æ–‡{report_type}\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {ts} (UTC+8)\n")
            if self.hours_ago:
                cutoff = now - timedelta(hours=self.hours_ago)
                if cutoff.date() == now.date():
                    win = f"{cutoff.strftime('%H:%M')} ~ {now.strftime('%H:%M')}"
                else:
                    win = f"{cutoff.strftime('%mæœˆ%dæ—¥ %H:%M')} ~ {now.strftime('%mæœˆ%dæ—¥ %H:%M')}"
                f.write(f"**æ—¶é—´çª—å£**: {win}\n")
            f.write(f"**AI ç›¸å…³æ¨æ–‡**: {following_count} æ¡å…³æ³¨ + {len(trending_tweets)} æ¡çƒ­é—¨\n")
            f.write(f"**æ€»è®¡**: {total} æ¡\n\n")

            if summary:
                f.write("---\n\n")
                f.write(summary)
                f.write("\n\n")

            if followings_data:
                f.write("---\n\n")
                f.write("# å…³æ³¨åˆ—è¡¨ AI æ¨æ–‡\n\n")
                for user_data in followings_data:
                    user = user_data["user"]
                    tweets = user_data["tweets"]
                    uname = user.get("username", "")
                    name = user.get("name", "")
                    desc = user.get("description", "")
                    followers = user.get("public_metrics", {}).get("followers_count", 0)
                    f.write(f"## @{uname} ({name})\n\n")
                    if desc:
                        f.write(f"> {desc}\n\n")
                    f.write(f"**ç²‰ä¸**: {followers:,}\n\n")
                    for tweet in tweets:
                        self._write_tweet_md(f, tweet)
                    f.write("---\n\n")

            if trending_tweets:
                f.write("---\n\n")
                f.write("# å…¨ç½‘çƒ­é—¨ AI æ¨æ–‡\n\n")
                for tweet in trending_tweets:
                    self._write_tweet_md(f, tweet)
                    f.write("---\n\n")

    # â”€â”€ æ¨é€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _telegram_send(self, text):
        """å‘é€æ¶ˆæ¯åˆ° Telegram"""
        push_config = self.config.get("push", {})
        tg = push_config.get("telegram", {})
        bot_token = tg.get("bot_token", "")
        chat_id = tg.get("chat_id", "")
        if not bot_token or not chat_id:
            return False
        try:
            resp = requests.post(
                f"https://api.telegram.org/bot{bot_token}/sendMessage",
                json={
                    "chat_id": chat_id,
                    "text": text,
                    "parse_mode": "Markdown",
                    "disable_web_page_preview": True,
                },
                timeout=15,
            )
            if resp.status_code == 200:
                return True
            print(f"  Telegram æ¨é€å¤±è´¥: {resp.status_code} {resp.text[:100]}")
            return False
        except Exception as e:
            print(f"  Telegram æ¨é€å¼‚å¸¸: {e}")
            return False

    def push_summary(self, source=None, test=False):
        """æ¨é€åˆ†ææ‘˜è¦åˆ° Telegram"""
        push_config = self.config.get("push", {})
        if not push_config.get("enabled", False) and not test:
            return

        if test:
            ok = self._telegram_send("âœ… Twitter Watchdog æ¨é€æµ‹è¯•æˆåŠŸï¼")
            print(f"  Telegram æµ‹è¯•: {'æˆåŠŸ' if ok else 'å¤±è´¥'}")
            return

        # è¯»å– analysis æ–‡ä»¶
        if source:
            source_path = Path(source)
        else:
            output_path = Path(self.output_config["directory"])
            analysis_dir = output_path / "analysis"
            if not analysis_dir.exists():
                return
            files = sorted(analysis_dir.glob("*.json"))
            if not files:
                return
            source_path = files[-1]

        with open(source_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        summary = data.get("summary", "")
        if not summary:
            return

        # æå–"æœ¬æœŸè¦ç‚¹"éƒ¨åˆ†
        highlights = self._extract_highlights(summary)
        if not highlights:
            highlights = summary[:1000]

        ts = self.now().strftime("%m/%d %H:%M")
        msg = f"ğŸ“¡ *AI æ–°é—»é€Ÿé€’* ({ts})\n\n{highlights}"

        # Telegram æ¶ˆæ¯é™åˆ¶ 4096 å­—ç¬¦
        if len(msg) > 4000:
            msg = msg[:3997] + "..."

        ok = self._telegram_send(msg)
        if ok:
            print(f"  ğŸ“¤ å·²æ¨é€åˆ° Telegram")

    def push_urgent(self, tweets):
        """æ¨é€çªå‘æ¨æ–‡åˆ° Telegram"""
        push_config = self.config.get("push", {})
        if not push_config.get("enabled", False):
            return
        for t in tweets:
            text = t.get("text", "")[:200]
            url = t.get("url", "")
            author = t.get("author", {}).get("userName", "")
            msg = f"ğŸ”´ *çªå‘ AI æ–°é—»*\n\n@{author}: {text}\n\n{url}"
            self._telegram_send(msg)

    @staticmethod
    def _extract_highlights(summary):
        """ä» summary ä¸­æå–"æœ¬æœŸè¦ç‚¹"éƒ¨åˆ†"""
        lines = summary.split("\n")
        in_highlights = False
        result = []
        for line in lines:
            if "æœ¬æœŸè¦ç‚¹" in line:
                in_highlights = True
                continue
            if in_highlights:
                if line.startswith("## "):
                    break
                if line.strip():
                    result.append(line)
        return "\n".join(result) if result else ""

    # â”€â”€ æµæ°´çº¿ï¼ˆå‘åå…¼å®¹ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run_pipeline(self):
        """ä¸‰æ­¥æµæ°´çº¿ï¼ˆå‘åå…¼å®¹æ—§çš„æ— å­å‘½ä»¤ç”¨æ³•ï¼‰"""
        raw_file = self.run_scrape()
        if not raw_file:
            print("\næŠ“å–æ— æ•°æ®ï¼Œæµæ°´çº¿ç»ˆæ­¢")
            return

        analysis_file = self.run_analyze(source=raw_file)
        if not analysis_file:
            print("\nåˆ†ææ— ç»“æœï¼Œæµæ°´çº¿ç»ˆæ­¢")
            return

        self.run_report(source=analysis_file)
        self.push_summary(source=analysis_file)

    # â”€â”€ æŠ¥å‘Šè¾“å‡º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _html_page(title, subtitle, body_html):
        """ç”Ÿæˆè‡ªåŒ…å« HTML é¡µé¢ï¼ˆsticky å¯¼èˆª + ä¸»é¢˜åˆ‡æ¢ + æš—è‰²æ¨¡å¼ + å›åˆ°é¡¶éƒ¨ï¼‰"""
        return f"""<!DOCTYPE html>
<html lang="zh-CN" data-theme="auto">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>{title}</title>
<style>
  :root {{
    --font-sans: -apple-system, BlinkMacSystemFont, "SF Pro Display", "SF Pro Text", "Segoe UI", Roboto, "Helvetica Neue", sans-serif;
    --radius-sm: 8px; --radius-md: 12px; --radius-lg: 16px;
    --shadow-sm: 0 1px 2px rgba(0,0,0,0.04), 0 1px 3px rgba(0,0,0,0.03);
    --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 1px 4px rgba(0,0,0,0.04);
    --transition-fast: 0.15s cubic-bezier(0.4, 0, 0.2, 1);
    --transition-med: 0.25s cubic-bezier(0.4, 0, 0.2, 1);
  }}
  [data-theme="light"], :root {{
    --bg-primary: #fafaf9; --bg-secondary: #ffffff; --bg-tertiary: #f5f5f4;
    --bg-nav: rgba(250,250,249,0.82);
    --text-primary: #1c1917; --text-secondary: #57534e; --text-tertiary: #a8a29e;
    --border-primary: #e7e5e4; --border-secondary: #d6d3d1;
    --accent: #4338ca; --accent-hover: #3730a3; --accent-subtle: rgba(67,56,202,0.08);
    --highlight-bg: linear-gradient(135deg, #faf5ff 0%, #f0f9ff 50%, #f0fdf4 100%);
    --highlight-border: #e9d5ff;
    --tag-product-bg: #eff6ff; --tag-product-text: #1e40af; --tag-product-border: #bfdbfe;
    --tag-model-bg: #fdf2f8; --tag-model-text: #9d174d; --tag-model-border: #fbcfe8;
    --tag-dev-bg: #ecfdf5; --tag-dev-text: #065f46; --tag-dev-border: #a7f3d0;
    --tag-industry-bg: #fffbeb; --tag-industry-text: #92400e; --tag-industry-border: #fde68a;
    --tag-research-bg: #f5f3ff; --tag-research-text: #5b21b6; --tag-research-border: #ddd6fe;
    --dot-1: #6366f1; --dot-2: #8b5cf6; --dot-3: #a78bfa; --dot-4: #c084fc; --dot-5: #6366f1;
    --back-top-bg: rgba(255,255,255,0.9); --back-top-shadow: 0 2px 12px rgba(0,0,0,0.1);
    --scrollbar-thumb: #d6d3d1;
  }}
  [data-theme="dark"] {{
    --bg-primary: #0c0a09; --bg-secondary: #1c1917; --bg-tertiary: #292524;
    --bg-nav: rgba(12,10,9,0.82);
    --text-primary: #fafaf9; --text-secondary: #a8a29e; --text-tertiary: #78716c;
    --border-primary: #292524; --border-secondary: #44403c;
    --accent: #818cf8; --accent-hover: #a5b4fc; --accent-subtle: rgba(129,140,248,0.1);
    --highlight-bg: linear-gradient(135deg, rgba(88,28,135,0.15) 0%, rgba(30,58,138,0.15) 50%, rgba(6,78,59,0.12) 100%);
    --highlight-border: #581c87;
    --tag-product-bg: rgba(37,99,235,0.15); --tag-product-text: #93c5fd; --tag-product-border: rgba(37,99,235,0.3);
    --tag-model-bg: rgba(219,39,119,0.15); --tag-model-text: #f9a8d4; --tag-model-border: rgba(219,39,119,0.3);
    --tag-dev-bg: rgba(5,150,105,0.15); --tag-dev-text: #6ee7b7; --tag-dev-border: rgba(5,150,105,0.3);
    --tag-industry-bg: rgba(217,119,6,0.15); --tag-industry-text: #fcd34d; --tag-industry-border: rgba(217,119,6,0.3);
    --tag-research-bg: rgba(124,58,237,0.15); --tag-research-text: #c4b5fd; --tag-research-border: rgba(124,58,237,0.3);
    --dot-1: #818cf8; --dot-2: #a78bfa; --dot-3: #c084fc; --dot-4: #e879f9; --dot-5: #818cf8;
    --back-top-bg: rgba(28,25,23,0.9); --back-top-shadow: 0 2px 12px rgba(0,0,0,0.4);
    --scrollbar-thumb: #44403c;
  }}
  @media (prefers-color-scheme: dark) {{
    [data-theme="auto"] {{
      --bg-primary: #0c0a09; --bg-secondary: #1c1917; --bg-tertiary: #292524;
      --bg-nav: rgba(12,10,9,0.82);
      --text-primary: #fafaf9; --text-secondary: #a8a29e; --text-tertiary: #78716c;
      --border-primary: #292524; --border-secondary: #44403c;
      --accent: #818cf8; --accent-hover: #a5b4fc; --accent-subtle: rgba(129,140,248,0.1);
      --highlight-bg: linear-gradient(135deg, rgba(88,28,135,0.15) 0%, rgba(30,58,138,0.15) 50%, rgba(6,78,59,0.12) 100%);
      --highlight-border: #581c87;
      --tag-product-bg: rgba(37,99,235,0.15); --tag-product-text: #93c5fd; --tag-product-border: rgba(37,99,235,0.3);
      --tag-model-bg: rgba(219,39,119,0.15); --tag-model-text: #f9a8d4; --tag-model-border: rgba(219,39,119,0.3);
      --tag-dev-bg: rgba(5,150,105,0.15); --tag-dev-text: #6ee7b7; --tag-dev-border: rgba(5,150,105,0.3);
      --tag-industry-bg: rgba(217,119,6,0.15); --tag-industry-text: #fcd34d; --tag-industry-border: rgba(217,119,6,0.3);
      --tag-research-bg: rgba(124,58,237,0.15); --tag-research-text: #c4b5fd; --tag-research-border: rgba(124,58,237,0.3);
      --dot-1: #818cf8; --dot-2: #a78bfa; --dot-3: #c084fc; --dot-4: #e879f9; --dot-5: #818cf8;
      --back-top-bg: rgba(28,25,23,0.9); --back-top-shadow: 0 2px 12px rgba(0,0,0,0.4);
      --scrollbar-thumb: #44403c;
    }}
  }}
  *, *::before, *::after {{ margin: 0; padding: 0; box-sizing: border-box; }}
  html {{ scroll-behavior: smooth; -webkit-text-size-adjust: 100%; }}
  body {{
    font-family: var(--font-sans); background: var(--bg-primary); color: var(--text-primary);
    line-height: 1.7; -webkit-font-smoothing: antialiased; overflow-x: hidden;
  }}
  ::-webkit-scrollbar {{ width: 6px; height: 6px; }}
  ::-webkit-scrollbar-track {{ background: transparent; }}
  ::-webkit-scrollbar-thumb {{ background: var(--scrollbar-thumb); border-radius: 3px; }}
  .page-container {{ max-width: 860px; margin: 0 auto; padding: 0 28px; }}
  .sticky-nav {{
    position: sticky; top: 0; z-index: 1000;
    background: var(--bg-nav); backdrop-filter: blur(16px) saturate(180%);
    -webkit-backdrop-filter: blur(16px) saturate(180%);
    border-bottom: 1px solid var(--border-primary); transition: box-shadow var(--transition-med);
  }}
  .sticky-nav.scrolled {{ box-shadow: 0 1px 8px rgba(0,0,0,0.06); }}
  .sticky-nav-inner {{
    max-width: 860px; margin: 0 auto; padding: 0 28px;
    display: flex; align-items: center; gap: 6px; height: 52px;
    overflow-x: auto; -webkit-overflow-scrolling: touch; scrollbar-width: none;
  }}
  .sticky-nav-inner::-webkit-scrollbar {{ display: none; }}
  .nav-brand {{ font-size: 14px; font-weight: 700; letter-spacing: -0.3px; color: var(--text-primary); white-space: nowrap; flex-shrink: 0; padding-right: 8px; }}
  .nav-divider {{ width: 1px; height: 18px; background: var(--border-secondary); flex-shrink: 0; margin: 0 4px; }}
  .nav-link {{
    display: inline-flex; align-items: center; padding: 10px 14px; border-radius: 99px;
    font-size: 13px; font-weight: 500; text-decoration: none; color: var(--text-secondary);
    background: transparent; border: 1px solid transparent; white-space: nowrap;
    flex-shrink: 0; cursor: pointer; transition: all var(--transition-fast); user-select: none;
    min-height: 44px;
  }}
  .nav-link:hover {{ color: var(--text-primary); background: var(--bg-tertiary); }}
  .nav-link.active {{ color: var(--accent); background: var(--accent-subtle); font-weight: 600; }}
  .nav-actions {{ margin-left: auto; flex-shrink: 0; display: flex; align-items: center; gap: 4px; }}
  .theme-toggle {{
    width: 34px; height: 34px; border-radius: 50%; border: 1px solid var(--border-primary);
    background: var(--bg-secondary); color: var(--text-secondary); cursor: pointer;
    display: flex; align-items: center; justify-content: center; transition: all var(--transition-fast); flex-shrink: 0;
  }}
  .theme-toggle:hover {{ border-color: var(--border-secondary); color: var(--text-primary); box-shadow: var(--shadow-sm); }}
  .theme-toggle svg {{ width: 16px; height: 16px; }}
  .theme-toggle .icon-moon, .theme-toggle .icon-sun {{ display: none; }}
  [data-resolved-theme="light"] .theme-toggle .icon-moon {{ display: block; }}
  [data-resolved-theme="dark"] .theme-toggle .icon-sun {{ display: block; }}
  .page-header {{ padding: 48px 0 40px; border-bottom: 1px solid var(--border-primary); margin-bottom: 40px; }}
  .page-header h1 {{ font-size: 32px; font-weight: 800; letter-spacing: -0.8px; line-height: 1.2; }}
  .page-header .subtitle {{ margin-top: 10px; font-size: 15px; color: var(--text-secondary); }}
  .page-header .date-badge {{
    display: inline-flex; align-items: center; gap: 6px; margin-top: 14px;
    padding: 5px 12px; border-radius: 99px; background: var(--accent-subtle);
    color: var(--accent); font-size: 12px; font-weight: 600; letter-spacing: 0.2px;
  }}
  .page-header .date-badge svg {{ width: 14px; height: 14px; }}
  .highlights-section {{ margin-bottom: 48px; }}
  .highlights-card {{
    background: var(--highlight-bg); border: 1px solid var(--highlight-border);
    border-radius: var(--radius-lg); padding: 28px 28px 20px; position: relative; overflow: hidden;
  }}
  .highlights-card::before {{
    content: ""; position: absolute; top: 0; left: 0; right: 0; height: 3px;
    background: linear-gradient(90deg, #6366f1, #8b5cf6, #a78bfa, #c084fc, #e879f9);
    border-radius: 3px 3px 0 0;
  }}
  .highlights-title {{ font-size: 16px; font-weight: 700; margin-bottom: 18px; display: flex; align-items: center; gap: 8px; }}
  .highlights-title svg {{ width: 18px; height: 18px; color: var(--accent); }}
  .highlights-list {{ list-style: none; display: flex; flex-direction: column; }}
  .highlights-list li {{
    position: relative; padding: 12px 0 12px 24px; font-size: 14.5px; line-height: 1.65;
    border-bottom: 1px solid rgba(0,0,0,0.06);
  }}
  [data-resolved-theme="dark"] .highlights-list li {{ border-bottom-color: rgba(255,255,255,0.06); }}
  .highlights-list li:last-child {{ border-bottom: none; padding-bottom: 4px; }}
  .highlights-list li::before {{ content: ""; position: absolute; left: 2px; top: 19px; width: 7px; height: 7px; border-radius: 50%; }}
  .highlights-list li:nth-child(1)::before {{ background: var(--dot-1); }}
  .highlights-list li:nth-child(2)::before {{ background: var(--dot-2); }}
  .highlights-list li:nth-child(3)::before {{ background: var(--dot-3); }}
  .highlights-list li:nth-child(4)::before {{ background: var(--dot-4); }}
  .highlights-list li:nth-child(5)::before {{ background: var(--dot-5); }}
  .category-section {{ margin-bottom: 48px; }}
  .category-section-header {{
    display: flex; align-items: center; gap: 12px;
    margin-bottom: 20px; padding-bottom: 14px; border-bottom: 1px solid var(--border-primary);
  }}
  .category-section-header h2 {{ font-size: 20px; font-weight: 700; letter-spacing: -0.3px; }}
  .cat-tag {{
    font-size: 11px; font-weight: 600; padding: 3px 10px; border-radius: 99px;
    letter-spacing: 0.4px; text-transform: uppercase; border: 1px solid;
  }}
  .cat-tag-product {{ background: var(--tag-product-bg); color: var(--tag-product-text); border-color: var(--tag-product-border); }}
  .cat-tag-model {{ background: var(--tag-model-bg); color: var(--tag-model-text); border-color: var(--tag-model-border); }}
  .cat-tag-dev {{ background: var(--tag-dev-bg); color: var(--tag-dev-text); border-color: var(--tag-dev-border); }}
  .cat-tag-industry {{ background: var(--tag-industry-bg); color: var(--tag-industry-text); border-color: var(--tag-industry-border); }}
  .cat-tag-research {{ background: var(--tag-research-bg); color: var(--tag-research-text); border-color: var(--tag-research-border); }}
  .category-section-header .item-count {{ font-size: 12px; color: var(--text-tertiary); margin-left: auto; }}
  .news-card {{
    background: var(--bg-secondary); border: 1px solid var(--border-primary);
    border-radius: var(--radius-md); padding: 22px 24px; margin-bottom: 14px;
    transition: all var(--transition-fast); position: relative;
  }}
  .news-card:hover {{ border-color: var(--border-secondary); box-shadow: var(--shadow-md); transform: translateY(-1px); }}
  .news-card:last-child {{ margin-bottom: 0; }}
  .news-card-title {{ font-size: 15.5px; font-weight: 600; line-height: 1.5; margin-bottom: 8px; letter-spacing: -0.2px; }}
  .news-card-title a {{ color: var(--accent); text-decoration: none; transition: all var(--transition-fast); }}
  .news-card-title a:hover {{ text-decoration: underline; text-underline-offset: 3px; text-decoration-thickness: 1.5px; }}
  .news-card-title a::after {{ content: " \\2197"; font-size: 12px; color: var(--text-tertiary); font-weight: 400; transition: color var(--transition-fast); }}
  .news-card-title a:hover::after {{ color: var(--accent); }}
  .news-card-desc {{ font-size: 14px; line-height: 1.7; color: var(--text-secondary); }}
  .news-card-img {{ margin-top: 16px; border-radius: var(--radius-sm); overflow: hidden; text-align: center; }}
  .news-card-img img {{ max-width: 100%; max-height: 400px; object-fit: contain; border-radius: var(--radius-sm); display: block; margin: 0 auto; }}
  .page-footer {{ margin-top: 56px; padding: 24px 0; border-top: 1px solid var(--border-primary); text-align: center; }}
  .page-footer p {{ font-size: 13px; color: var(--text-tertiary); line-height: 1.6; }}
  .footer-dot {{ display: inline-block; width: 3px; height: 3px; background: var(--text-tertiary); border-radius: 50%; vertical-align: middle; margin: 0 10px; }}
  .back-to-top {{
    position: fixed; bottom: 28px; right: 28px; width: 40px; height: 40px;
    border-radius: 50%; background: var(--back-top-bg); border: 1px solid var(--border-primary);
    color: var(--text-secondary); cursor: pointer; display: flex; align-items: center; justify-content: center;
    box-shadow: var(--back-top-shadow); backdrop-filter: blur(8px); -webkit-backdrop-filter: blur(8px);
    transition: all var(--transition-fast); opacity: 0; visibility: hidden; transform: translateY(8px); z-index: 999;
  }}
  .back-to-top.visible {{ opacity: 1; visibility: visible; transform: translateY(0); }}
  .back-to-top:hover {{ border-color: var(--border-secondary); color: var(--accent); transform: translateY(-2px); box-shadow: 0 4px 20px rgba(0,0,0,0.12); }}
  .back-to-top svg {{ width: 18px; height: 18px; }}
  @media (max-width: 640px) {{
    .page-container {{ padding: 0 16px; }}
    .sticky-nav-inner {{ padding: 0 16px; height: 52px; }}
    .page-header {{ padding: 28px 0 24px; margin-bottom: 24px; }}
    .page-header h1 {{ font-size: 22px; }}
    .page-header .subtitle {{ font-size: 14px; }}
    .highlights-card {{ padding: 20px 18px 14px; }}
    .highlights-list li {{ font-size: 14px; }}
    .category-section {{ margin-bottom: 32px; }}
    .category-section-header h2 {{ font-size: 18px; }}
    .news-card {{ padding: 16px; margin-bottom: 12px; }}
    .news-card-title {{ font-size: 15px; }}
    .news-card-desc {{ font-size: 14px; }}
    .news-card-img img {{ max-height: 300px; }}
    .back-to-top {{ bottom: 20px; right: 16px; width: 44px; height: 44px; }}
    .theme-toggle {{ width: 44px; height: 44px; }}
    .nav-actions {{ margin-left: 4px; }}
  }}
  @media print {{
    .sticky-nav, .back-to-top, .theme-toggle {{ display: none !important; }}
    body {{ background: #fff; color: #000; }}
    .news-card {{ break-inside: avoid; border: 1px solid #ddd; }}
  }}
</style>
</head>
<body>
<nav class="sticky-nav" id="sticky-nav">
  <div class="sticky-nav-inner" id="sticky-nav-inner">
    <span class="nav-brand">AI æ—¥æŠ¥</span>
    <span class="nav-divider"></span>
    <div class="nav-actions">
      <button class="theme-toggle" id="theme-toggle" type="button" aria-label="åˆ‡æ¢ä¸»é¢˜" title="åˆ‡æ¢æ·±è‰²/æµ…è‰²æ¨¡å¼">
        <svg class="icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
        <svg class="icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
      </button>
    </div>
  </div>
</nav>
<div class="page-container">
<header class="page-header">
  <h1>{title}</h1>
  <div class="subtitle">äººå·¥æ™ºèƒ½é¢†åŸŸä»Šæ—¥è¦é—»é€Ÿè§ˆ</div>
  <div class="date-badge">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
    {subtitle}
  </div>
</header>
{body_html}
</div>
<footer class="page-footer">
  <div class="page-container">
    <p>ç”± Twitter Watchdog è‡ªåŠ¨ç”Ÿæˆ<span class="footer-dot"></span>æ•°æ®æ¥æº: Twitter å…³æ³¨åˆ—è¡¨ + å…¨ç½‘çƒ­é—¨</p>
  </div>
</footer>
<button class="back-to-top" id="back-to-top" type="button" aria-label="è¿”å›é¡¶éƒ¨" title="è¿”å›é¡¶éƒ¨">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="18 15 12 9 6 15"/></svg>
</button>
<script>
(function() {{
  var html = document.documentElement;
  var themeBtn = document.getElementById('theme-toggle');
  function getSystemTheme() {{ return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light'; }}
  function resolveTheme() {{ var t = html.getAttribute('data-theme'); return t === 'auto' ? getSystemTheme() : t; }}
  function applyResolvedTheme() {{ html.setAttribute('data-resolved-theme', resolveTheme()); }}
  applyResolvedTheme();
  window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', function() {{ applyResolvedTheme(); }});
  themeBtn.addEventListener('click', function() {{
    var next = resolveTheme() === 'light' ? 'dark' : 'light';
    html.setAttribute('data-theme', next); applyResolvedTheme();
  }});
  var navOuter = document.getElementById('sticky-nav');
  var navInner = document.getElementById('sticky-nav-inner');
  var sections = document.querySelectorAll('[data-nav]');
  var navActions = navInner.querySelector('.nav-actions');
  sections.forEach(function(sec) {{
    var a = document.createElement('a');
    a.className = 'nav-link'; a.href = '#' + sec.id;
    a.textContent = sec.getAttribute('data-nav');
    a.setAttribute('data-target', sec.id);
    a.addEventListener('click', function(e) {{
      e.preventDefault();
      var target = document.getElementById(this.getAttribute('data-target'));
      if (target) {{
        var navH = navOuter.offsetHeight;
        var top = target.getBoundingClientRect().top + window.pageYOffset - navH - 16;
        window.scrollTo({{ top: top, behavior: 'smooth' }});
      }}
    }});
    navInner.insertBefore(a, navActions);
  }});
  var navLinks = navInner.querySelectorAll('.nav-link[data-target]');
  function scrollNavTo(el) {{
    var r = navInner.getBoundingClientRect(), e = el.getBoundingClientRect();
    navInner.scrollBy({{ left: e.left - r.left - (r.width / 2) + (e.width / 2), behavior: 'smooth' }});
  }}
  if (navLinks.length) {{
    var observer = new IntersectionObserver(function(entries) {{
      entries.forEach(function(entry) {{
        if (entry.isIntersecting) {{
          navLinks.forEach(function(l) {{ l.classList.remove('active'); }});
          var active = navInner.querySelector('.nav-link[data-target="' + entry.target.id + '"]');
          if (active) {{ active.classList.add('active'); scrollNavTo(active); }}
        }}
      }});
    }}, {{ rootMargin: '-80px 0px -60% 0px', threshold: 0 }});
    sections.forEach(function(sec) {{ observer.observe(sec); }});
  }}
  window.addEventListener('scroll', function() {{
    if (window.pageYOffset > 10) navOuter.classList.add('scrolled');
    else navOuter.classList.remove('scrolled');
  }}, {{ passive: true }});
  var btt = document.getElementById('back-to-top');
  window.addEventListener('scroll', function() {{
    if (window.pageYOffset > 400) btt.classList.add('visible');
    else btt.classList.remove('visible');
  }}, {{ passive: true }});
  btt.addEventListener('click', function(e) {{ e.preventDefault(); window.scrollTo({{ top: 0, behavior: 'smooth' }}); }});
}})();
</script>
</body>
</html>"""

    def save_as_html(self, output_file, ai_summary_with_images, timestamp):
        """å°† AI æ€»ç»“è½¬ä¸ºè‡ªåŒ…å« HTML é¡µé¢"""
        now = self.now()
        date_str = now.strftime("%Y å¹´ %m æœˆ %d æ—¥")
        window_desc = ""
        if self.hours_ago:
            cutoff = now - timedelta(hours=self.hours_ago)
            if cutoff.date() == now.date():
                window_desc = f"{cutoff.strftime('%H:%M')} ~ {now.strftime('%H:%M')}"
            else:
                window_desc = f"{cutoff.strftime('%mæœˆ%dæ—¥ %H:%M')} ~ {now.strftime('%mæœˆ%dæ—¥ %H:%M')}"

        body_html = self._summary_md_to_html(ai_summary_with_images or "æš‚æ— å†…å®¹")
        title = f"AI æ—¥æŠ¥ â€” {date_str}"
        subtitle = f"{date_str} {window_desc}"
        html = self._html_page(title, subtitle, body_html)
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(html)

    def _summary_md_to_html(self, md_text):
        """å°†åˆ†ç±»ç»“æ„çš„ markdown æ€»ç»“è½¬ä¸º HTMLï¼ˆåŒ¹é…æ–°ç‰ˆ UI æ¨¡æ¿ï¼‰"""
        import html as html_mod

        # æ˜Ÿæ ‡ SVG å›¾æ ‡
        star_svg = '<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polygon points="12 2 15.09 8.26 22 9.27 17 14.14 18.18 21.02 12 17.77 5.82 21.02 7 14.14 2 9.27 8.91 8.26 12 2"/></svg>'

        category_tags = {
            "AI äº§å“ä¸å·¥å…·": ("product", "cat-tag-product", "Product"),
            "AI æ¨¡å‹ä¸æŠ€æœ¯": ("model", "cat-tag-model", "Model"),
            "AI å¼€å‘è€…ç”Ÿæ€": ("dev", "cat-tag-dev", "Dev"),
            "AI è¡Œä¸šåŠ¨æ€": ("industry", "cat-tag-industry", "Industry"),
            "AI ç ”ç©¶ä¸è§‚ç‚¹": ("research", "cat-tag-research", "Research"),
        }

        short_labels = {
            "AI äº§å“ä¸å·¥å…·": "äº§å“ä¸å·¥å…·",
            "AI æ¨¡å‹ä¸æŠ€æœ¯": "æ¨¡å‹ä¸æŠ€æœ¯",
            "AI å¼€å‘è€…ç”Ÿæ€": "å¼€å‘è€…ç”Ÿæ€",
            "AI è¡Œä¸šåŠ¨æ€": "è¡Œä¸šåŠ¨æ€",
            "AI ç ”ç©¶ä¸è§‚ç‚¹": "ç ”ç©¶ä¸è§‚ç‚¹",
        }

        sections = []
        current_title = None
        current_lines = []

        for line in md_text.split("\n"):
            if line.startswith("## "):
                if current_title is not None:
                    sections.append((current_title, current_lines))
                current_title = line[3:].strip()
                current_lines = []
            else:
                current_lines.append(line)
        if current_title is not None:
            sections.append((current_title, current_lines))

        parts = []

        for title, lines in sections:
            if title == "æœ¬æœŸè¦ç‚¹":
                bullets = [l.lstrip("- ").strip() for l in lines if l.strip().startswith("- ")]
                if bullets:
                    parts.append(f'<section class="highlights-section" id="highlights" data-nav="æœ¬æœŸè¦ç‚¹">')
                    parts.append(f'<div class="highlights-card">')
                    parts.append(f'<div class="highlights-title">{star_svg} æœ¬æœŸè¦ç‚¹</div>')
                    parts.append('<ul class="highlights-list">')
                    for b in bullets:
                        parts.append(f"<li>{html_mod.escape(b)}</li>")
                    parts.append("</ul></div></section>")
                continue

            tag_info = category_tags.get(title)
            if not tag_info:
                continue
            tag_id, tag_class, tag_label = tag_info
            anchor = f"cat-{tag_id}"
            nav_label = short_labels.get(title, title)

            # ç»Ÿè®¡æ¡ç›®æ•°
            item_count = sum(1 for l in lines if l.strip().startswith("- ["))

            parts.append(f'<section class="category-section" id="{anchor}" data-nav="{html_mod.escape(nav_label)}">')
            parts.append(f'<div class="category-section-header">'
                         f'<h2>{html_mod.escape(title)}</h2>'
                         f'<span class="cat-tag {tag_class}">{tag_label}</span>'
                         f'<span class="item-count">{item_count} æ¡</span></div>')

            i = 0
            while i < len(lines):
                line = lines[i].strip()
                if line.startswith("- ["):
                    m = re.match(r'^- \[(.+?)\]\((.+?)\)[ã€‚ï¼Œ,.]\s*(.*)$', line, re.DOTALL)
                    if m:
                        item_title, item_url, item_desc = m.groups()
                        img_html = ""
                        j = i + 1
                        while j < len(lines):
                            sl = lines[j].strip()
                            if sl.startswith("!["):
                                img_m = re.match(r'!\[.*?\]\((.+?)\)', sl)
                                if img_m:
                                    img_src = img_m.group(1)
                                    img_html = f'<div class="news-card-img"><img src="{html_mod.escape(img_src)}" alt="" loading="lazy"></div>'
                                j += 1
                            elif sl == "":
                                j += 1
                            else:
                                break
                        i = j

                        parts.append('<div class="news-card">')
                        parts.append(f'<div class="news-card-title"><a href="{html_mod.escape(item_url)}" target="_blank" rel="noopener">{html_mod.escape(item_title)}</a></div>')
                        if item_desc.strip():
                            parts.append(f'<div class="news-card-desc">{html_mod.escape(item_desc.strip())}</div>')
                        if img_html:
                            parts.append(img_html)
                        parts.append('</div>')
                        continue
                i += 1

            parts.append("</section>")

        return "\n".join(parts)

    def _write_tweet_md(self, f, tweet):
        """å†™å…¥å•æ¡æ¨æ–‡çš„ Markdown"""
        created_raw = tweet.get("createdAt", "")
        created_cn = self.parse_tweet_time(created_raw)
        time_str = created_cn.strftime("%Y-%m-%d %H:%M") if created_cn else created_raw

        text = tweet.get("text", "")
        likes = tweet.get("likeCount", 0)
        retweets = tweet.get("retweetCount", 0)
        replies = tweet.get("replyCount", 0)
        views = tweet.get("viewCount", 0)
        url = tweet.get("url", "")
        author = tweet.get("author", {})
        author_name = author.get("userName", "") or author.get("name", "")

        f.write(f"### {time_str}")
        if author_name:
            f.write(f" Â· @{author_name}")
        f.write("\n\n")
        f.write(f"{text}\n\n")
        f.write(f"*{replies} replies | {retweets} retweets | {likes} likes | {views} views*")
        if url:
            f.write(f"  [åŸæ–‡é“¾æ¥]({url})")
        f.write("\n\n")

    # â”€â”€ å‘¨æŠ¥/æœˆæŠ¥è¾…åŠ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _parse_summary_items(self, summary_text):
        """è§£æ AI æ€»ç»“æ–‡æœ¬ï¼Œæå–æ¯æ¡æ–°é—»æ¡ç›®"""
        items = []
        if not summary_text:
            return items
        for para in summary_text.split("\n\n"):
            para = para.strip()
            if not para.startswith("- ["):
                continue
            match = re.match(r'^- \[(.+?)\]\((.+?)\)[ï¼Œã€‚,.]\s*(.+)$', para, re.DOTALL)
            if match:
                title, url, desc = match.groups()
                items.append({
                    "title": title,
                    "url": url,
                    "description": desc.strip(),
                    "full_text": para,
                })
        return items

    def _deduplicate_items(self, all_items):
        """æŒ‰ URL å»é‡ï¼Œä¿ç•™æè¿°æœ€é•¿çš„ç‰ˆæœ¬"""
        url_map = {}
        for item in all_items:
            url = item["url"]
            if url not in url_map or len(item["full_text"]) > len(url_map[url]["full_text"]):
                url_map[url] = item
        return list(url_map.values())

    def _claude_consolidate(self, items, period, period_label):
        """è°ƒç”¨ Claude å°†å»é‡åçš„æ¡ç›®æ•´åˆä¸ºå‘¨æŠ¥/æœˆæŠ¥"""
        summary_config = self.config.get("ai_summary", {})
        api_key = (
            summary_config.get("api_key", "")
            or os.environ.get("ANTHROPIC_AUTH_TOKEN", "")
            or os.environ.get("ANTHROPIC_API_KEY", "")
        )
        if not api_key:
            print("  è·³è¿‡ Claude æ•´åˆï¼ˆæœªé…ç½® API Keyï¼‰")
            return None

        base_url = (
            summary_config.get("base_url", "")
            or os.environ.get("ANTHROPIC_BASE_URL", "")
            or "https://api.anthropic.com"
        )

        label = "æœˆæŠ¥" if period == "monthly" else "å‘¨æŠ¥"
        content = "\n".join(item["full_text"] for item in items)

        prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI è¡Œä¸šä¿¡æ¯æ•´ç†å‘˜ã€‚ä»¥ä¸‹æ˜¯ {period_label} æœŸé—´æ¯æ—¥ AI æ¨æ–‡æ€»ç»“ä¸­æ±‡æ€»çš„ä¿¡æ¯æ¡ç›®ï¼ˆå·²æŒ‰ URL åˆæ­¥å»é‡ï¼Œå…± {len(items)} æ¡ï¼‰ã€‚

ä»»åŠ¡ï¼šå°†è¿™äº›æ¡ç›®æ•´åˆä¸ºä¸€ä»½ç»“æ„åŒ–çš„{label}ã€‚

è¾“å‡ºç»“æ„ï¼ˆä¸¥æ ¼éµå¾ªï¼‰ï¼š

## æœ¬æœŸè¦ç‚¹

ç”¨ 3~5 ä¸ªbullet point æ¦‚æ‹¬æœ¬æœŸæœ€é‡è¦çš„äº‹ä»¶/å‘å¸ƒ/è¶‹åŠ¿ï¼Œæ¯æ¡ä¸€å¥è¯ï¼Œä¸å¸¦é“¾æ¥ã€‚

## AI äº§å“ä¸å·¥å…·

æ–°äº§å“å‘å¸ƒã€äº§å“é‡å¤§æ›´æ–°ã€å·¥å…·æ¨èç­‰ã€‚

## AI æ¨¡å‹ä¸æŠ€æœ¯

æ–°æ¨¡å‹å‘å¸ƒã€æ¨¡å‹è¯„æµ‹ã€æŠ€æœ¯æ¶æ„ã€ç®—æ³•çªç ´ç­‰ã€‚

## AI å¼€å‘è€…ç”Ÿæ€

å¼€å‘æ¡†æ¶ã€APIã€SDKã€å¼€æºé¡¹ç›®ã€å¼€å‘è€…å·¥å…·é“¾ç­‰ã€‚

## AI è¡Œä¸šåŠ¨æ€

å…¬å¸æˆ˜ç•¥ã€èèµ„æ”¶è´­ã€äººäº‹å˜åŠ¨ã€æ”¿ç­–æ³•è§„ã€è¡Œä¸šåˆä½œç­‰ã€‚

## AI ç ”ç©¶ä¸è§‚ç‚¹

å­¦æœ¯è®ºæ–‡ã€å®éªŒç»“æœã€è¡Œä¸šè§‚å¯Ÿã€è¶‹åŠ¿åˆ†æç­‰ã€‚

æ¯ä¸ªåˆ†ç±»ä¸‹çš„æ¡ç›®æ ¼å¼ï¼š
- [å…·ä½“æ ‡é¢˜](æ¨æ–‡URL)ã€‚å®¢è§‚æè¿°ï¼Œä¿¡æ¯é½å…¨ä½†ä¸å†—ä½™ã€‚

è§„åˆ™ï¼š
- åˆå¹¶æŠ¥é“åŒä¸€äº‹ä»¶/äº§å“çš„ä¸åŒæ¡ç›®ï¼Œä¿ç•™æœ€å®Œæ•´çš„æè¿°
- æ¯ä¸ªåˆ†ç±»å†…æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½æ’åˆ—
- æ¯æ¡æè¿°åº”åŒ…å«å…³é”®æ•°æ®ã€æ ¸å¿ƒåŠŸèƒ½ã€å…·ä½“ç‰¹ç‚¹ï¼Œå»é™¤é‡å¤ä¿®é¥°å’Œç©ºæ³›è¡¨è¿°
- åªæè¿°å®¢è§‚äº‹å®ï¼Œä¸åšä¸»è§‚è¯„ä»·
- å¦‚æœæŸä¸ªåˆ†ç±»ä¸‹æ²¡æœ‰å†…å®¹ï¼Œçœç•¥è¯¥åˆ†ç±»
- ä¸è¦åŠ ç»Ÿè®¡æ•°æ®æˆ–ç»“å°¾æ€»ç»“

---
{content}"""

        model = summary_config.get("model", "claude-sonnet-4-5-20250929")
        max_tokens = summary_config.get("max_tokens", 4096)
        if period == "monthly":
            max_tokens = max(max_tokens, 8192)

        print(f"  è°ƒç”¨ Claude ({model}) æ•´åˆ{label}...")
        try:
            api_url = f"{base_url.rstrip('/')}/v1/messages"
            resp = requests.post(
                api_url,
                headers={
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json",
                },
                json={
                    "model": model,
                    "max_tokens": max_tokens,
                    "messages": [{"role": "user", "content": prompt}],
                },
                timeout=180,
            )
            resp.raise_for_status()
            result = resp.json()
            text = result["content"][0]["text"]
            usage = result.get("usage", {})
            print(f"  Claude æ•´åˆå®Œæˆï¼ˆ{usage.get('input_tokens', 0)} + {usage.get('output_tokens', 0)} tokensï¼‰")
            return text
        except Exception as e:
            print(f"  Claude æ•´åˆå¤±è´¥: {e}")
            return None



def main():
    parser = argparse.ArgumentParser(
        description="Twitter Watchdog - AI æ¨æ–‡ç›‘æ§å·¥å…·ï¼ˆä¸‰å±‚æ¶æ„ï¼šæŠ“å– â†’ åˆ†æ â†’ æŠ¥å‘Šï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç”¨æ³•ç¤ºä¾‹ï¼š
  # Layer 1: åªæŠ“å–ï¼Œå­˜åŸå§‹æ•°æ®
  python3 twitter_watchdog.py scrape --hours-ago 6

  # Layer 2: åˆ†æåŸå§‹æ•°æ®ï¼Œç”Ÿæˆåˆ†æç»“æœ
  python3 twitter_watchdog.py analyze --hours-ago 6
  python3 twitter_watchdog.py analyze --source raw/20260212_140000.json
  python3 twitter_watchdog.py analyze --from "2026-02-12 08:00" --to "2026-02-12 14:00"

  # Layer 3: ä»åˆ†æç»“æœç”ŸæˆæŠ¥å‘Š
  python3 twitter_watchdog.py report
  python3 twitter_watchdog.py report --source analysis/20260212_143000.json
  python3 twitter_watchdog.py report --daily 2026-02-12
  python3 twitter_watchdog.py report --weekly 2026-02-10
  python3 twitter_watchdog.py report --monthly 2026-02

  # æµæ°´çº¿æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼Œç­‰ä»·äº scrape + analyze + reportï¼‰
  python3 twitter_watchdog.py --hours-ago 6
""",
    )
    # é¡¶å±‚å‚æ•°ï¼ˆå…¼å®¹æ—§ç”¨æ³• + æ‰€æœ‰å­å‘½ä»¤å…±ç”¨ï¼‰
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-dir", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--hours-ago", type=int, help="æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰")
    parser.add_argument("--max-followings", type=int, help="å…³æ³¨åˆ—è¡¨æŠ“å–èŒƒå›´ï¼ˆ0=å…¨éƒ¨ï¼‰")
    parser.add_argument("--tweets-per-user", type=int, help="æ¯ä¸ªç”¨æˆ·æœ€å¤šæ¨æ–‡æ•°")
    parser.add_argument("--trending-count", type=int, help="çƒ­é—¨æ¨æ–‡æœ€å¤šæ¡æ•°")
    parser.add_argument("--trending-query", help="çƒ­é—¨æœç´¢å…³é”®è¯ï¼ˆTwitter æœç´¢è¯­æ³•ï¼‰")
    parser.add_argument("--min-faves", type=int, help="çƒ­é—¨æ¨æ–‡æœ€ä½æµè§ˆé‡")
    parser.add_argument("--language", help="è¯­è¨€è¿‡æ»¤ï¼ˆall/en/zh/ja...ï¼‰")
    parser.add_argument("--exclude-users", help="æ’é™¤çš„ç”¨æˆ·åï¼ˆé€—å·åˆ†éš”ï¼‰")
    parser.add_argument("--reset-state", action="store_true", help="é‡ç½®å»é‡çŠ¶æ€")
    parser.add_argument("--no-trending", action="store_true", help="ç¦ç”¨çƒ­é—¨æœç´¢")
    parser.add_argument("--no-summary", action="store_true", help="ç¦ç”¨ AI æ€»ç»“")

    subparsers = parser.add_subparsers(dest="command")

    # scrape
    subparsers.add_parser("scrape", help="Layer 1: æŠ“å–æ¨æ–‡åŸå§‹æ•°æ®")

    # analyze
    sp_analyze = subparsers.add_parser("analyze", help="Layer 2: AI åˆ†æåŸå§‹æ•°æ®")
    sp_analyze.add_argument("--source", help="æŒ‡å®š raw JSON æ–‡ä»¶è·¯å¾„")
    sp_analyze.add_argument("--from", dest="time_from", help="èµ·å§‹æ—¶é—´ï¼ˆå¦‚ '2026-02-12 08:00'ï¼‰")
    sp_analyze.add_argument("--to", dest="time_to", help="ç»“æŸæ—¶é—´ï¼ˆå¦‚ '2026-02-12 14:00'ï¼‰")

    # report
    sp_report = subparsers.add_parser("report", help="Layer 3: ç”ŸæˆæŠ¥å‘Š")
    sp_report.add_argument("--source", help="æŒ‡å®š analysis JSON æ–‡ä»¶è·¯å¾„")
    sp_report.add_argument("--daily", metavar="YYYY-MM-DD", help="ç”Ÿæˆæ—¥æŠ¥")
    sp_report.add_argument("--weekly", metavar="YYYY-MM-DD", help="ç”Ÿæˆå‘¨æŠ¥ï¼ˆä»æŒ‡å®šæ—¥æœŸèµ· 7 å¤©ï¼‰")
    sp_report.add_argument("--monthly", metavar="YYYY-MM", help="ç”ŸæˆæœˆæŠ¥")

    # push
    sp_push = subparsers.add_parser("push", help="æ¨é€æ‘˜è¦åˆ° Telegram")
    sp_push.add_argument("--source", help="æŒ‡å®š analysis JSON æ–‡ä»¶è·¯å¾„")
    sp_push.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨é€é…ç½®")

    args = parser.parse_args()

    # é‡ç½®çŠ¶æ€
    if args.reset_state:
        state_file = ".twitter_watchdog_state.json"
        if os.path.exists(state_file):
            os.remove(state_file)
            print("å·²é‡ç½®å»é‡çŠ¶æ€")
        else:
            print("æ— çŠ¶æ€æ–‡ä»¶ï¼Œæ— éœ€é‡ç½®")

    # è·¯ç”±
    if args.command == "scrape":
        watchdog = TwitterWatchdog(config_file=args.config, cli_args=args)
        watchdog.run_scrape()

    elif args.command == "analyze":
        watchdog = TwitterWatchdog(config_file=args.config, cli_args=args, report_only=True)
        time_from = None
        time_to = None
        if getattr(args, "time_from", None):
            time_from = datetime.strptime(args.time_from, "%Y-%m-%d %H:%M").replace(tzinfo=TZ_CN)
        if getattr(args, "time_to", None):
            time_to = datetime.strptime(args.time_to, "%Y-%m-%d %H:%M").replace(tzinfo=TZ_CN)
        watchdog.run_analyze(
            source=getattr(args, "source", None),
            time_from=time_from,
            time_to=time_to,
        )

    elif args.command == "report":
        watchdog = TwitterWatchdog(config_file=args.config, cli_args=args, report_only=True)
        watchdog.run_report(
            source=getattr(args, "source", None),
            daily=getattr(args, "daily", None),
            weekly=getattr(args, "weekly", None),
            monthly=getattr(args, "monthly", None),
        )

    elif args.command == "push":
        watchdog = TwitterWatchdog(config_file=args.config, cli_args=args, report_only=True)
        if getattr(args, "test", False):
            watchdog.push_summary(test=True)
        else:
            watchdog.push_summary(source=getattr(args, "source", None))

    else:
        # æ— å­å‘½ä»¤ â†’ æµæ°´çº¿ï¼ˆå‘åå…¼å®¹ï¼‰
        watchdog = TwitterWatchdog(config_file=args.config, cli_args=args)
        watchdog.run_pipeline()

    print("\n=== å®Œæˆ ===")


if __name__ == "__main__":
    main()
