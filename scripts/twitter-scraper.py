#!/usr/bin/env python3
"""
Twitter API - å®Œå…¨ç‹¬ç«‹ç‰ˆæœ¬
ä¸ä¾èµ–ç¯å¢ƒå˜é‡ï¼Œç›´æ¥ä½¿ç”¨æ­£ç¡®çš„ Token
"""

import requests
import base64
import json
import argparse

# ç›´æ¥ä½¿ç”¨å‡­æ®
CONSUMER_KEY = "k1aM91JgDj4obSMSsM2KWGSss"
CONSUMER_SECRET = "yNha9rF5VcF5mD5aMBkWrY5wWO9En1BWtEQWa4KVJzBsCfpJuo"

# é…ç½®ä»£ç†
PROXIES = {
    'http': 'http://127.0.0.1:7890',
    'https': 'http://127.0.0.1:7890'
}

def get_oauth_bearer_token():
    """è·å– OAuth 2.0 Bearer Tokenï¼ˆURL ç¼–ç ç‰ˆæœ¬ï¼‰"""
    session = requests.Session()
    session.proxies = PROXIES
    
    oauth_url = "https://api.twitter.com/oauth2/token"
    credentials = f"{CONSUMER_KEY}:{CONSUMER_SECRET}"
    b64_credentials = base64.b64encode(credentials.encode()).decode()
    
    headers = {
        "Authorization": f"Basic {b64_credentials}",
        "Content-Type": "application/x-www-form-urlencoded"
    }
    
    data = "grant_type=client_credentials"
    
    response = session.post(oauth_url, headers=headers, data=data, timeout=10)
    
    if response.status_code == 200:
        result = response.json()
        # è¿”å› URL ç¼–ç çš„ Tokenï¼ˆè¿™æ˜¯æ­£ç¡®çš„ï¼‰
        return result.get("access_token"), session
    else:
        raise Exception(f"è·å– Bearer Token å¤±è´¥: {response.text}")

def get_user_by_username(username):
    """è·å–ç”¨æˆ·ä¿¡æ¯"""
    bearer_token, session = get_oauth_bearer_token()
    
    url = f"https://api.twitter.com/2/users/by/username/{username}"
    headers = {
        "Authorization": f"Bearer {bearer_token}",
        "Content-Type": "application/json"
    }
    
    response = session.get(url, headers=headers, timeout=10)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"è·å–ç”¨æˆ·å¤±è´¥: {response.text}")

def get_user_tweets(user_id, max_results=100):
    """è·å–ç”¨æˆ·æ¨æ–‡"""
    bearer_token, session = get_oauth_bearer_token()
    
    url = f"https://api.twitter.com/2/users/{user_id}/tweets"
    params = {
        "max_results": max_results,
        "tweet.fields": "created_at,author_id,public_metrics"
    }
    headers = {
        "Authorization": f"Bearer {bearer_token}",
        "Content-Type": "application/json"
    }
    
    all_tweets = []
    next_token = None
    
    while True:
        if next_token:
            params["pagination_token"] = next_token
        
        response = session.get(url, headers=headers, params=params, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            tweets = data.get("data", [])
            all_tweets.extend(tweets)
            
            meta = data.get("meta", {})
            next_token = meta.get("next_token")
            
            if not next_token:
                break
                
            print(f"âœ… å·²è·å– {len(all_tweets)} æ¡æ¨æ–‡...")
        else:
            raise Exception(f"è·å–æ¨æ–‡å¤±è´¥: {response.text}")
    
    return all_tweets

def search_tweets(query, max_results=100):
    """æœç´¢æ¨æ–‡"""
    bearer_token, session = get_oauth_bearer_token()
    
    url = "https://api.twitter.com/2/tweets/search/recent"
    params = {
        "query": query,
        "max_results": max_results,
        "tweet.fields": "created_at,author_id,public_metrics"
    }
    headers = {
        "Authorization": f"Bearer {bearer_token}",
        "Content-Type": "application/json"
    }
    
    response = session.get(url, headers=headers, params=params, timeout=10)
    
    if response.status_code == 200:
        data = response.json()
        return data.get("data", [])
    else:
        raise Exception(f"æœç´¢å¤±è´¥: {response.text}")

def save_json(data, filename):
    """ä¿å­˜ JSON"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²ä¿å­˜åˆ° {filename}")

def save_csv(tweets, filename):
    """ä¿å­˜ CSV"""
    import csv
    
    if not tweets:
        print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return
    
    fieldnames = ['id', 'text', 'created_at', 'author_id']
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for tweet in tweets:
            writer.writerow({
                'id': tweet.get('id', ''),
                'text': tweet.get('text', ''),
                'created_at': tweet.get('created_at', ''),
                'author_id': tweet.get('author_id', '')
            })
    
    print(f"âœ… å·²ä¿å­˜åˆ° {filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Twitter API æŠ“å–å™¨')
    parser.add_argument('--username', help='ç”¨æˆ·å')
    parser.add_argument('--search', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--max-results', type=int, default=2, help='æœ€å¤§ç»“æœæ•°')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    try:
        if args.search:
            print(f"ğŸ” æœç´¢: {args.search}")
            tweets = search_tweets(args.search, args.max_results)
            
            if tweets:
                print(f"âœ… æ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡")
                
                for i, tweet in enumerate(tweets[:5], 1):
                    text = tweet.get('text', '')
                    created = tweet.get('created_at', '')
                    print(f"\n{i}. [{created}] {text[:100]}...")
                
                if args.output:
                    if args.output.endswith('.csv'):
                        save_csv(tweets, args.output)
                    else:
                        save_json(tweets, args.output)
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
        
        elif args.username:
            print(f"ğŸ‘¤ è·å–ç”¨æˆ·: @{args.username}")
            
            user_data = get_user_by_username(args.username)
            user_info = user_data.get('data', {})
            
            print(f"âœ… ç”¨æˆ· ID: {user_info.get('id')}")
            print(f"   åç§°: {user_info.get('name')}")
            print(f"   ç”¨æˆ·å: @{user_info.get('username')}")
            
            user_id = user_info.get('id')
            
            if user_id:
                tweets = get_user_tweets(user_id, args.max_results)
                
                if tweets:
                    print(f"\nâœ… è·å–åˆ° {len(tweets)} æ¡æ¨æ–‡")
                    
                    for i, tweet in enumerate(tweets[:5], 1):
                        text = tweet.get('text', '')
                        created = tweet.get('created_at', '')
                        print(f"\n{i}. [{created}] {text[:100]}...")
                    
                    if args.output:
                        if args.output.endswith('.csv'):
                            save_csv(tweets, args.output)
                        else:
                            save_json(tweets, args.output)
        
        else:
            print("âš ï¸ è¯·æŒ‡å®š --username æˆ– --search")
            print("ç¤ºä¾‹:")
            print("  python3 scripts/twitter-scraper.py --username elonmusk")
            print("  python3 scripts/twitter-scraper.py --search 'AI' --max-results 10")
    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
